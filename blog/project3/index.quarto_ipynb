{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Logit Model\"\n",
        "author: \"Eleanor Amidei\"\n",
        "date: today\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data. \n",
        "*Student note* this was professor provided, but I changed it to Python because I don't have R installed. And I won't.\n"
      ],
      "id": "2c62de11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Define attributes\n",
        "brand = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\n",
        "ad = [\"Yes\", \"No\"]\n",
        "price = np.arange(8, 33, 4)\n",
        "\n",
        "# Generate all possible profiles\n",
        "profiles = pd.DataFrame(\n",
        "    [(b, a, p) for b in brand for a in ad for p in price],\n",
        "    columns=[\"brand\", \"ad\", \"price\"]\n",
        ")\n",
        "m = len(profiles)\n",
        "\n",
        "# Assign part-worth utilities (true parameters)\n",
        "b_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\n",
        "a_util = {\"Yes\": -0.8, \"No\": 0.0}\n",
        "def p_util(p):\n",
        "    return -0.1 * p\n",
        "\n",
        "# Number of respondents, choice tasks, and alternatives per task\n",
        "n_peeps = 100\n",
        "n_tasks = 10\n",
        "n_alts = 3\n",
        "\n",
        "# Function to simulate one respondent's data\n",
        "def sim_one(resp_id):\n",
        "    datlist = []\n",
        "    for t in range(1, n_tasks + 1):\n",
        "        sampled_profiles = profiles.sample(n=n_alts).copy()\n",
        "        sampled_profiles.insert(0, \"task\", t)\n",
        "        sampled_profiles.insert(0, \"resp\", resp_id)\n",
        "\n",
        "        # Compute deterministic portion of utility\n",
        "        v = sampled_profiles[\"brand\"].map(b_util) + \\\n",
        "            sampled_profiles[\"ad\"].map(a_util) + \\\n",
        "            p_util(sampled_profiles[\"price\"])\n",
        "\n",
        "        # Add Gumbel noise (Type I extreme value)\n",
        "        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n",
        "        u = v + e\n",
        "\n",
        "        # Identify chosen alternative\n",
        "        choice = (u == u.max()).astype(int)\n",
        "\n",
        "        sampled_profiles[\"choice\"] = choice\n",
        "        datlist.append(sampled_profiles)\n",
        "\n",
        "    return pd.concat(datlist, ignore_index=True)\n",
        "\n",
        "# Simulate data for all respondents\n",
        "conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n",
        "\n",
        "# Keep only observable columns\n",
        "conjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]"
      ],
      "id": "ada748ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "s\n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n",
        "\n",
        "_todo: reshape and prep the data_"
      ],
      "id": "f0450532"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "conjoint_data = pd.read_csv(\"conjoint_data.csv\")\n",
        "\n",
        "# Convert categorical variables to binary indicators\n",
        "conjoint_data['netflix'] = (conjoint_data['brand'] == 'N').astype(int)\n",
        "conjoint_data['prime'] = (conjoint_data['brand'] == 'P').astype(int)\n",
        "conjoint_data['ads'] = (conjoint_data['ad'] == 'Yes').astype(int)\n",
        "\n",
        "# Create design matrix X\n",
        "X = conjoint_data[['netflix', 'prime', 'ads', 'price']].values\n",
        "y = conjoint_data['choice'].values\n",
        "\n",
        "# Create task identifiers\n",
        "conjoint_data['resp_task'] = conjoint_data['resp'].astype(str) + \"_\" + conjoint_data['task'].astype(str)\n",
        "task_ids = conjoint_data['resp_task'].values\n",
        "\n",
        "conjoint_data.head()\n"
      ],
      "id": "a73a93ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        "_todo: Code up the log-likelihood function._\n",
        "\n",
        "_todo: Use `optim()` in R or `scipy.optimize()` in Python to find the MLEs for the 4 parameters ($\\beta_\\text{netflix}$, $\\beta_\\text{prime}$, $\\beta_\\text{ads}$, $\\beta_\\text{price}$), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval._\n"
      ],
      "id": "f1fff3a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def log_likelihood_mnl(beta, X, y, task_ids):\n",
        "    \"\"\"Calculate log-likelihood for MNL model\"\"\"\n",
        "    # Calculate linear predictors\n",
        "    V = X @ beta\n",
        "    \n",
        "    ll = 0\n",
        "    unique_tasks = np.unique(task_ids)\n",
        "    \n",
        "    for task in unique_tasks:\n",
        "        # Get indices for this task\n",
        "        task_idx = (task_ids == task)\n",
        "        V_task = V[task_idx]\n",
        "        y_task = y[task_idx]\n",
        "        \n",
        "        # Calculate choice probabilities using softmax\n",
        "        V_max = np.max(V_task)  # for numerical stability\n",
        "        exp_V = np.exp(V_task - V_max)\n",
        "        probs = exp_V / np.sum(exp_V)\n",
        "        \n",
        "        # Add to log-likelihood (only for chosen alternative)\n",
        "        chosen_idx = np.where(y_task == 1)[0]\n",
        "        if len(chosen_idx) == 1:\n",
        "            ll += np.log(probs[chosen_idx[0]])\n",
        "    \n",
        "    return ll\n",
        "def neg_log_likelihood(beta, X, y, task_ids):\n",
        "    \"\"\"Negative log-likelihood for optimization\"\"\"\n",
        "    return -log_likelihood_mnl(beta, X, y, task_ids)\n",
        "\n",
        "# Starting values\n",
        "beta_start = np.array([0.0, 0.0, 0.0, 0.0])\n",
        "\n",
        "# Maximum likelihood estimation\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "mle_result = minimize(\n",
        "    neg_log_likelihood,\n",
        "    beta_start,\n",
        "    args=(X, y, task_ids),\n",
        "    method='BFGS',\n",
        "    options={'disp': False}\n",
        ")\n",
        "\n",
        "# Extract MLE estimates\n",
        "beta_mle = mle_result.x\n",
        "param_names = ['netflix', 'prime', 'ads', 'price']\n",
        "\n",
        "# Calculate Hessian numerically for standard errors\n",
        "def hessian_numerical(f, x, args, h=1e-5):\n",
        "    \"\"\"Calculate numerical Hessian\"\"\"\n",
        "    n = len(x)\n",
        "    H = np.zeros((n, n))\n",
        "    \n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            x_pp = x.copy(); x_pp[i] += h; x_pp[j] += h\n",
        "            x_pm = x.copy(); x_pm[i] += h; x_pm[j] -= h\n",
        "            x_mp = x.copy(); x_mp[i] -= h; x_mp[j] += h\n",
        "            x_mm = x.copy(); x_mm[i] -= h; x_mm[j] -= h\n",
        "            \n",
        "            H[i,j] = (f(x_pp, *args) - f(x_pm, *args) - f(x_mp, *args) + f(x_mm, *args)) / (4 * h**2)\n",
        "    \n",
        "    return H\n",
        "\n",
        "# Calculate Hessian and standard errors\n",
        "hessian = hessian_numerical(neg_log_likelihood, beta_mle, (X, y, task_ids))\n",
        "se_mle = np.sqrt(np.diag(np.linalg.inv(hessian)))\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "ci_lower = beta_mle - 1.96 * se_mle\n",
        "ci_upper = beta_mle + 1.96 * se_mle\n",
        "\n",
        "# Display MLE results\n",
        "print(\"\\n=== MAXIMUM LIKELIHOOD ESTIMATION RESULTS ===\")\n",
        "print(\"True parameters: Netflix=1.0, Prime=0.5, Ads=-0.8, Price=-0.1\\n\")\n",
        "\n",
        "mle_results_df = pd.DataFrame({\n",
        "    'Parameter': param_names,\n",
        "    'Estimate': np.round(beta_mle, 4),\n",
        "    'SE': np.round(se_mle, 4),\n",
        "    'CI_Lower': np.round(ci_lower, 4),\n",
        "    'CI_Upper': np.round(ci_upper, 4)\n",
        "})\n",
        "print(mle_results_df.to_string(index=False))\n"
      ],
      "id": "0b05d704",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Estimation via Bayesian Methods\n",
        "\n",
        "_todo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000._\n",
        "\n",
        "_hint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta._\n",
        "\n",
        "_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\n",
        "\n",
        "_hint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands.  Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous.  So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal.  Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005)._\n",
        "\n",
        "\n",
        "_todo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution._\n",
        "\n",
        "_todo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach._\n"
      ],
      "id": "97ff3c95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def log_prior(beta):\n",
        "    \"\"\"Log-prior function with specified priors\"\"\"\n",
        "    # N(0,5) priors for binary variables (netflix, prime, ads)\n",
        "    # N(0,1) prior for price\n",
        "    ll_prior = (norm.logpdf(beta[0], 0, 5) +\n",
        "                norm.logpdf(beta[1], 0, 5) +\n",
        "                norm.logpdf(beta[2], 0, 5) +\n",
        "                norm.logpdf(beta[3], 0, 1))\n",
        "    return ll_prior\n",
        "\n",
        "def log_posterior(beta, X, y, task_ids):\n",
        "    \"\"\"Log-posterior function\"\"\"\n",
        "    ll = log_likelihood_mnl(beta, X, y, task_ids)\n",
        "    lp = log_prior(beta)\n",
        "    return ll + lp\n",
        "\n",
        "def metropolis_hastings_mcmc(n_iter, X, y, task_ids, beta_init=None):\n",
        "    \"\"\"Metropolis-Hastings MCMC sampler\"\"\"\n",
        "    \n",
        "    # Initialize\n",
        "    if beta_init is None:\n",
        "        beta_current = np.array([0.0, 0.0, 0.0, 0.0])\n",
        "    else:\n",
        "        beta_current = beta_init.copy()\n",
        "    \n",
        "    # Proposal covariance (diagonal as suggested)\n",
        "    proposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n",
        "    \n",
        "    # Storage for samples\n",
        "    samples = np.zeros((n_iter, 4))\n",
        "    n_accepted = 0\n",
        "    \n",
        "    # Current log-posterior\n",
        "    log_post_current = log_posterior(beta_current, X, y, task_ids)\n",
        "    \n",
        "    print(\"Starting MCMC sampling...\")\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "        # Propose new state\n",
        "        beta_proposed = beta_current + np.random.normal(0, proposal_sd, 4)\n",
        "        \n",
        "        # Calculate log-posterior for proposed state\n",
        "        try:\n",
        "            log_post_proposed = log_posterior(beta_proposed, X, y, task_ids)\n",
        "            \n",
        "            # Calculate acceptance probability\n",
        "            log_alpha = min(0, log_post_proposed - log_post_current)\n",
        "            \n",
        "            # Accept or reject\n",
        "            if np.log(np.random.rand()) < log_alpha:\n",
        "                beta_current = beta_proposed\n",
        "                log_post_current = log_post_proposed\n",
        "                n_accepted += 1\n",
        "            \n",
        "        except:\n",
        "            # If calculation fails, reject proposal\n",
        "            pass\n",
        "        \n",
        "        # Store sample\n",
        "        samples[i] = beta_current\n",
        "        \n",
        "        # Print progress\n",
        "        if (i + 1) % 1000 == 0:\n",
        "            acceptance_rate = n_accepted / (i + 1)\n",
        "            print(f\"Iteration {i+1:5d}, Acceptance rate: {acceptance_rate:.3f}\")\n",
        "    \n",
        "    acceptance_rate = n_accepted / n_iter\n",
        "    print(f\"Final acceptance rate: {acceptance_rate:.3f}\")\n",
        "    \n",
        "    return samples, acceptance_rate\n",
        "\n",
        "# Run MCMC\n",
        "n_total = 11000\n",
        "n_burnin = 1000\n",
        "\n",
        "# Start from MLE estimates for better convergence\n",
        "samples_all, acceptance_rate = metropolis_hastings_mcmc(\n",
        "    n_total, X, y, task_ids, beta_init=beta_mle\n",
        ")\n",
        "\n",
        "# Remove burn-in\n",
        "samples = samples_all[n_burnin:]\n",
        "n_keep = len(samples)\n",
        "\n",
        "print(f\"\\nMCMC complete! Kept {n_keep} samples after burn-in.\")\n",
        "\n",
        "# Calculate posterior statistics\n",
        "posterior_means = np.mean(samples, axis=0)\n",
        "posterior_sds = np.std(samples, axis=0)\n",
        "posterior_ci_lower = np.percentile(samples, 2.5, axis=0)\n",
        "posterior_ci_upper = np.percentile(samples, 97.5, axis=0)\n",
        "\n",
        "# Display Bayesian results\n",
        "print(\"\\n=== BAYESIAN MCMC ESTIMATION RESULTS ===\")\n",
        "bayesian_results_df = pd.DataFrame({\n",
        "    'Parameter': param_names,\n",
        "    'Post_Mean': np.round(posterior_means, 4),\n",
        "    'Post_SD': np.round(posterior_sds, 4),\n",
        "    'CI_Lower': np.round(posterior_ci_lower, 4),\n",
        "    'CI_Upper': np.round(posterior_ci_upper, 4)\n",
        "})\n",
        "print(bayesian_results_df.to_string(index=False))"
      ],
      "id": "2a3c13f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create trace plots and histograms for Netflix parameter (index 0)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Netflix parameter\n",
        "param_idx = 0\n",
        "param_name = param_names[param_idx]\n",
        "\n",
        "# Trace plot\n",
        "axes[0, 0].plot(samples[:, param_idx])\n",
        "axes[0, 0].set_title(f'Trace Plot: {param_name.title()}')\n",
        "axes[0, 0].set_xlabel('Iteration')\n",
        "axes[0, 0].set_ylabel('Parameter Value')\n",
        "axes[0, 0].axhline(y=1.0, color='red', linestyle='--', label='True Value')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Histogram\n",
        "axes[0, 1].hist(samples[:, param_idx], bins=50, density=True, alpha=0.7)\n",
        "axes[0, 1].axvline(x=1.0, color='red', linestyle='--', label='True Value')\n",
        "axes[0, 1].axvline(x=posterior_means[param_idx], color='blue', linestyle='-', label='Posterior Mean')\n",
        "axes[0, 1].set_title(f'Posterior Distribution: {param_name.title()}')\n",
        "axes[0, 1].set_xlabel('Parameter Value')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Price parameter (more interesting because it's on different scale)\n",
        "param_idx = 3\n",
        "param_name = param_names[param_idx]\n",
        "\n",
        "# Trace plot\n",
        "axes[1, 0].plot(samples[:, param_idx])\n",
        "axes[1, 0].set_title(f'Trace Plot: {param_name.title()}')\n",
        "axes[1, 0].set_xlabel('Iteration')\n",
        "axes[1, 0].set_ylabel('Parameter Value')\n",
        "axes[1, 0].axhline(y=-0.1, color='red', linestyle='--', label='True Value')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Histogram\n",
        "axes[1, 1].hist(samples[:, param_idx], bins=50, density=True, alpha=0.7)\n",
        "axes[1, 1].axvline(x=-0.1, color='red', linestyle='--', label='True Value')\n",
        "axes[1, 1].axvline(x=posterior_means[param_idx], color='blue', linestyle='-', label='Posterior Mean')\n",
        "axes[1, 1].set_title(f'Posterior Distribution: {param_name.title()}')\n",
        "axes[1, 1].set_xlabel('Parameter Value')\n",
        "axes[1, 1].set_ylabel('Density')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "65fbc5e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Discussion\n",
        "\n",
        "_todo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does $\\beta_\\text{Netflix} > \\beta_\\text{Prime}$ mean? Does it make sense that $\\beta_\\text{price}$ is negative?_\n",
        "\n",
        "_todo: At a high level, discuss what change you would need to make in order to simulate data from --- and estimate the parameters of --- a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze \"real world\" conjoint data._\n"
      ],
      "id": "edc683db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n=== COMPARISON OF MLE AND BAYESIAN RESULTS ===\")\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Parameter': param_names,\n",
        "    'True_Value': [1.0, 0.5, -0.8, -0.1],\n",
        "    'MLE_Estimate': np.round(beta_mle, 4),\n",
        "    'MLE_SE': np.round(se_mle, 4),\n",
        "    'Bayes_Mean': np.round(posterior_means, 4),\n",
        "    'Bayes_SD': np.round(posterior_sds, 4)\n",
        "})\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== DISCUSSION POINTS ===\")\n",
        "print(\"1. Parameter Interpretation:\")\n",
        "print(f\"   - Netflix coefficient ({beta_mle[0]:.3f}) > Prime coefficient ({beta_mle[1]:.3f})\")\n",
        "print(\"     This suggests consumers prefer Netflix over Amazon Prime, with Hulu as the reference.\")\n",
        "print(\"   - Negative price coefficient suggests consumers prefer lower prices (as expected).\")\n",
        "print(\"   - Negative ads coefficient suggests consumers prefer ad-free content.\")\n",
        "\n",
        "print(f\"\\n2. Model Performance:\")\n",
        "print(f\"   - Both MLE and Bayesian approaches recover parameters close to true values\")\n",
        "print(f\"   - MCMC acceptance rate: {acceptance_rate:.3f} (reasonable for this problem)\")\n",
        "print(f\"   - Standard errors from both methods are quite similar\")\n",
        "\n",
        "print(f\"\\n3. For hierarchical/multi-level models:\")\n",
        "print(\"   - Would need to add individual-level random effects: β_i = β + u_i\")\n",
        "print(\"   - This requires modeling the distribution of u_i (typically multivariate normal)\")\n",
        "print(\"   - Estimation becomes more complex, often requiring specialized software\")\n",
        "print(\"   - Allows for heterogeneity in preferences across consumers\")"
      ],
      "id": "53eaaa4c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python311",
      "language": "python",
      "display_name": "Python 3.11",
      "path": "/Users/eleanor/Library/Jupyter/kernels/python311"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}