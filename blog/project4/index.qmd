---
title: Machine Learning
author: Eleanor Amidei
date: today
---

_todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._


## 1a. K-Means

_todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working.  Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables.  Compare your results to the built-in `kmeans` function in R or Python._

``` {python}
import pandas as pd
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Load dataset
penguins = pd.read_csv('palmer_penguins.csv')
data = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()

# Standardize features
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

``` 

``` {python}
import numpy as np
import matplotlib.pyplot as plt

def initialize_centroids(X, k):
    np.random.seed(42)
    indices = np.random.choice(X.shape[0], size=k, replace=False)
    return X[indices]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def kmeans(X, k, max_iters=100, tol=1e-4, animate=False):
    centroids = initialize_centroids(X, k)
    for i in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tol):
            break
        centroids = new_centroids
        if animate:
            plot_clusters(X, labels, centroids, iteration=i)
    return labels, centroids

def plot_clusters(X, labels, centroids, iteration):
    plt.figure()
    for i in np.unique(labels):
        plt.scatter(X[labels == i, 0], X[labels == i, 1], label=f'Cluster {i}')
    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')
    plt.title(f'Iteration {iteration}')
    plt.xlabel('Bill Length (standardized)')
    plt.ylabel('Flipper Length (standardized)')
    plt.legend()
    plt.show()


labels_custom, centroids_custom = kmeans(scaled_data, k=3, animate=True)

plot_clusters(scaled_data, labels_custom, centroids_custom, iteration='Final (Custom KMeans)')
```
Honestly, the model froms scratch looks like it did a pretty good job.

_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). What is the "right" number of clusters as suggested by these two metrics?_

_If you want a challenge, add your plots as an animated gif on your website so that the result looks something like [this](https://www.youtube.com/shorts/XCsoWZU9oN8)._

``` {python}
from sklearn.cluster import KMeans

kmeans_sklearn = KMeans(n_clusters=3, random_state=42)
kmeans_sklearn.fit(scaled_data)

plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=kmeans_sklearn.labels_)
plt.scatter(kmeans_sklearn.cluster_centers_[:, 0], kmeans_sklearn.cluster_centers_[:, 1], c='red', marker='x')
plt.title('Scikit-learn KMeans Result')
plt.xlabel('Bill Length (standardized)')
plt.ylabel('Flipper Length (standardized)')
plt.show()
```

The point where WCSS stops decreasing sharply indicates the best balance of compactness and simplicity

Elbow Method is showing that K=3 is the K while the Silhouette score shows K=2 to be the best with K=3 close. The higher the silhouette score the better the separation. So I would choose 3 because it's the best for both metrics

``` {python}
from sklearn.metrics import silhouette_score

wcss = []
silhouette_scores = []
K_range = range(2, 8)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(scaled_data)
    wcss.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(scaled_data, kmeans.labels_))

# Plot WCSS
plt.plot(K_range, wcss, marker='o')
plt.title('Elbow Method (WCSS)')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Within-Cluster Sum of Squares')
plt.show()

# Plot Silhouette Scores
plt.plot(K_range, silhouette_scores, marker='o')
plt.title('Silhouette Scores')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.show()

``` 


## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._

``` {python}
import pandas as pd

df = pd.read_csv('data_for_drivers_analysis.csv')

# Assume: last column is the target, all others are predictors
X = df[["trust", "build", "differs", "easy", "appealing", "rewarding", "popular", "service", "impact"]]
y = df["satisfaction"]

pearson_corr = X.corrwith(y).abs() * 100

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
y_scaled = (y - y.mean()) / y.std()

linreg = LinearRegression().fit(X_scaled, y_scaled)
std_coef = pd.Series(abs(linreg.coef_) * 100, index=X.columns)


from sklearn.metrics import r2_score

full_r2 = r2_score(y_scaled, linreg.predict(X_scaled))
usefulness = {}

for col in X.columns:
    subset = X.drop(columns=[col])
    subset_scaled = scaler.fit_transform(subset)
    model = LinearRegression().fit(subset_scaled, y_scaled)
    reduced_r2 = r2_score(y_scaled, model.predict(subset_scaled))
    usefulness[col] = (full_r2 - reduced_r2) * 100

usefulness = pd.Series(usefulness)


import shap

explainer = shap.Explainer(linreg, X_scaled)
shap_vals = explainer(X_scaled)
shap_importance = pd.DataFrame(shap_vals.values, columns=X.columns).abs().mean() * 100

import numpy as np

X_std = scaler.fit_transform(X)
# Step 1: Correlation matrix
R = np.corrcoef(X_std, rowvar=False)

# Step 2: Eigen decomposition
eig_vals, eig_vecs = np.linalg.eig(R)

# Step 3: Compute component loadings
loadings = eig_vecs @ np.diag(np.sqrt(eig_vals))

# Step 4: Square and normalize
raw_weights = np.sum(loadings**2 * (np.corrcoef(X_std.T, y)[-1, :-1]**2), axis=1)
relative_weights = 100 * raw_weights / raw_weights.sum()

# Step 5: Wrap up in DataFrame
johnson_weights = pd.Series(relative_weights, index=X.columns).round(2)


from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(random_state=42)
rf.fit(X, y)
rf_importance = pd.Series(rf.feature_importances_, index=X.columns) * 100

import xgboost as xgb

xgb_model = xgb.XGBRegressor(random_state=42)
xgb_model.fit(X, y)
xgb_importance = pd.Series(xgb_model.feature_importances_, index=X.columns) * 100

from sklearn.neural_network import MLPRegressor
from sklearn.inspection import permutation_importance

nn = MLPRegressor(hidden_layer_sizes=(50,), random_state=42, max_iter=1000)
nn.fit(X, y)
perm_imp = permutation_importance(nn, X, y, n_repeats=10, random_state=42)
nn_importance = pd.Series(perm_imp.importances_mean, index=X.columns) * 100

importance_table = pd.DataFrame({
    'Pearson Corr': pearson_corr,
    'Standardized Coef': std_coef,
    'Usefulness (Δ R²)': usefulness,
    'Shapley (Linear)': shap_importance,
    'Johnson Weights': johnson_weights,
    'RF Gini': rf_importance,
    'XGBoost': xgb_importance,                  # Optional
    'NN Permutation': nn_importance            # Optional
})

# Optional: Round values for readability
importance_table = importance_table.round(1)

# Sort if needed:
importance_table = importance_table.sort_values(by='Shapley (Linear)', ascending=False)
importance_table



