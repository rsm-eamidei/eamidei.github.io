[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nEleanor Amidei\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\nEleanor Amidei\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nEleanor Amidei\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nEleanor Amidei\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/index.html#simulate-conjoint-data",
    "href": "blog/project3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data. Student note this was professor provided, but I changed it to Python because I don’t have R installed. And I won’t.\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrand = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nad = [\"Yes\", \"No\"]\nprice = np.arange(8, 33, 4)\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [(b, a, p) for b in brand for a in ad for p in price],\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# Assign part-worth utilities (true parameters)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\ndef p_util(p):\n    return -0.1 * p\n\n# Number of respondents, choice tasks, and alternatives per task\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent's data\ndef sim_one(resp_id):\n    datlist = []\n    for t in range(1, n_tasks + 1):\n        sampled_profiles = profiles.sample(n=n_alts).copy()\n        sampled_profiles.insert(0, \"task\", t)\n        sampled_profiles.insert(0, \"resp\", resp_id)\n\n        # Compute deterministic portion of utility\n        v = sampled_profiles[\"brand\"].map(b_util) + \\\n            sampled_profiles[\"ad\"].map(a_util) + \\\n            p_util(sampled_profiles[\"price\"])\n\n        # Add Gumbel noise (Type I extreme value)\n        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        u = v + e\n\n        # Identify chosen alternative\n        choice = (u == u.max()).astype(int)\n\n        sampled_profiles[\"choice\"] = choice\n        datlist.append(sampled_profiles)\n\n    return pd.concat(datlist, ignore_index=True)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\n# Keep only observable columns\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\ns"
  },
  {
    "objectID": "blog/project3/index.html#preparing-the-data-for-estimation",
    "href": "blog/project3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data\n\nimport pandas as pd\nconjoint_data = pd.read_csv(\"conjoint_data.csv\")\n\n# Convert categorical variables to binary indicators\nconjoint_data['netflix'] = (conjoint_data['brand'] == 'N').astype(int)\nconjoint_data['prime'] = (conjoint_data['brand'] == 'P').astype(int)\nconjoint_data['ads'] = (conjoint_data['ad'] == 'Yes').astype(int)\n\n# Create design matrix X\nX = conjoint_data[['netflix', 'prime', 'ads', 'price']].values\ny = conjoint_data['choice'].values\n\n# Create task identifiers\nconjoint_data['resp_task'] = conjoint_data['resp'].astype(str) + \"_\" + conjoint_data['task'].astype(str)\ntask_ids = conjoint_data['resp_task'].values\n\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nbrand\nad\nprice\nnetflix\nprime\nads\nresp_task\n\n\n\n\n0\n1\n1\n1\nN\nYes\n28\n1\n0\n1\n1_1\n\n\n1\n1\n1\n0\nH\nYes\n16\n0\n0\n1\n1_1\n\n\n2\n1\n1\n0\nP\nYes\n16\n0\n1\n1\n1_1\n\n\n3\n1\n2\n0\nN\nYes\n32\n1\n0\n1\n1_2\n\n\n4\n1\n2\n1\nP\nYes\n16\n0\n1\n1\n1_2"
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval.\n\ndef log_likelihood_mnl(beta, X, y, task_ids):\n    \"\"\"Calculate log-likelihood for MNL model\"\"\"\n    # Calculate linear predictors\n    V = X @ beta\n    \n    ll = 0\n    unique_tasks = np.unique(task_ids)\n    \n    for task in unique_tasks:\n        # Get indices for this task\n        task_idx = (task_ids == task)\n        V_task = V[task_idx]\n        y_task = y[task_idx]\n        \n        # Calculate choice probabilities using softmax\n        V_max = np.max(V_task)  # for numerical stability\n        exp_V = np.exp(V_task - V_max)\n        probs = exp_V / np.sum(exp_V)\n        \n        # Add to log-likelihood (only for chosen alternative)\n        chosen_idx = np.where(y_task == 1)[0]\n        if len(chosen_idx) == 1:\n            ll += np.log(probs[chosen_idx[0]])\n    \n    return ll\ndef neg_log_likelihood(beta, X, y, task_ids):\n    \"\"\"Negative log-likelihood for optimization\"\"\"\n    return -log_likelihood_mnl(beta, X, y, task_ids)\n\n# Starting values\nbeta_start = np.array([0.0, 0.0, 0.0, 0.0])\n\n# Maximum likelihood estimation\nfrom scipy.optimize import minimize\n\nmle_result = minimize(\n    neg_log_likelihood,\n    beta_start,\n    args=(X, y, task_ids),\n    method='BFGS',\n    options={'disp': False}\n)\n\n# Extract MLE estimates\nbeta_mle = mle_result.x\nparam_names = ['netflix', 'prime', 'ads', 'price']\n\n# Calculate Hessian numerically for standard errors\ndef hessian_numerical(f, x, args, h=1e-5):\n    \"\"\"Calculate numerical Hessian\"\"\"\n    n = len(x)\n    H = np.zeros((n, n))\n    \n    for i in range(n):\n        for j in range(n):\n            x_pp = x.copy(); x_pp[i] += h; x_pp[j] += h\n            x_pm = x.copy(); x_pm[i] += h; x_pm[j] -= h\n            x_mp = x.copy(); x_mp[i] -= h; x_mp[j] += h\n            x_mm = x.copy(); x_mm[i] -= h; x_mm[j] -= h\n            \n            H[i,j] = (f(x_pp, *args) - f(x_pm, *args) - f(x_mp, *args) + f(x_mm, *args)) / (4 * h**2)\n    \n    return H\n\n# Calculate Hessian and standard errors\nhessian = hessian_numerical(neg_log_likelihood, beta_mle, (X, y, task_ids))\nse_mle = np.sqrt(np.diag(np.linalg.inv(hessian)))\n\n# Calculate 95% confidence intervals\nci_lower = beta_mle - 1.96 * se_mle\nci_upper = beta_mle + 1.96 * se_mle\n\n# Display MLE results\nprint(\"\\n=== MAXIMUM LIKELIHOOD ESTIMATION RESULTS ===\")\nprint(\"True parameters: Netflix=1.0, Prime=0.5, Ads=-0.8, Price=-0.1\\n\")\n\nmle_results_df = pd.DataFrame({\n    'Parameter': param_names,\n    'Estimate': np.round(beta_mle, 4),\n    'SE': np.round(se_mle, 4),\n    'CI_Lower': np.round(ci_lower, 4),\n    'CI_Upper': np.round(ci_upper, 4)\n})\nprint(mle_results_df.to_string(index=False))\n\n\n=== MAXIMUM LIKELIHOOD ESTIMATION RESULTS ===\nTrue parameters: Netflix=1.0, Prime=0.5, Ads=-0.8, Price=-0.1\n\nParameter  Estimate     SE  CI_Lower  CI_Upper\n  netflix    0.9412 0.1110    0.7236    1.1588\n    prime    0.5016 0.1111    0.2839    0.7194\n      ads   -0.7320 0.0878   -0.9041   -0.5599\n    price   -0.0995 0.0063   -0.1119   -0.0871"
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-bayesian-methods",
    "href": "blog/project3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach.\n\nfrom scipy.stats import norm\n\ndef log_prior(beta):\n    \"\"\"Log-prior function with specified priors\"\"\"\n    # N(0,5) priors for binary variables (netflix, prime, ads)\n    # N(0,1) prior for price\n    ll_prior = (norm.logpdf(beta[0], 0, 5) +\n                norm.logpdf(beta[1], 0, 5) +\n                norm.logpdf(beta[2], 0, 5) +\n                norm.logpdf(beta[3], 0, 1))\n    return ll_prior\n\ndef log_posterior(beta, X, y, task_ids):\n    \"\"\"Log-posterior function\"\"\"\n    ll = log_likelihood_mnl(beta, X, y, task_ids)\n    lp = log_prior(beta)\n    return ll + lp\n\ndef metropolis_hastings_mcmc(n_iter, X, y, task_ids, beta_init=None):\n    \"\"\"Metropolis-Hastings MCMC sampler\"\"\"\n    \n    # Initialize\n    if beta_init is None:\n        beta_current = np.array([0.0, 0.0, 0.0, 0.0])\n    else:\n        beta_current = beta_init.copy()\n    \n    # Proposal covariance (diagonal as suggested)\n    proposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n    \n    # Storage for samples\n    samples = np.zeros((n_iter, 4))\n    n_accepted = 0\n    \n    # Current log-posterior\n    log_post_current = log_posterior(beta_current, X, y, task_ids)\n    \n    print(\"Starting MCMC sampling...\")\n    \n    for i in range(n_iter):\n        # Propose new state\n        beta_proposed = beta_current + np.random.normal(0, proposal_sd, 4)\n        \n        # Calculate log-posterior for proposed state\n        try:\n            log_post_proposed = log_posterior(beta_proposed, X, y, task_ids)\n            \n            # Calculate acceptance probability\n            log_alpha = min(0, log_post_proposed - log_post_current)\n            \n            # Accept or reject\n            if np.log(np.random.rand()) &lt; log_alpha:\n                beta_current = beta_proposed\n                log_post_current = log_post_proposed\n                n_accepted += 1\n            \n        except:\n            # If calculation fails, reject proposal\n            pass\n        \n        # Store sample\n        samples[i] = beta_current\n        \n        # Print progress\n        if (i + 1) % 1000 == 0:\n            acceptance_rate = n_accepted / (i + 1)\n            print(f\"Iteration {i+1:5d}, Acceptance rate: {acceptance_rate:.3f}\")\n    \n    acceptance_rate = n_accepted / n_iter\n    print(f\"Final acceptance rate: {acceptance_rate:.3f}\")\n    \n    return samples, acceptance_rate\n\n# Run MCMC\nn_total = 11000\nn_burnin = 1000\n\n# Start from MLE estimates for better convergence\nsamples_all, acceptance_rate = metropolis_hastings_mcmc(\n    n_total, X, y, task_ids, beta_init=beta_mle\n)\n\n# Remove burn-in\nsamples = samples_all[n_burnin:]\nn_keep = len(samples)\n\nprint(f\"\\nMCMC complete! Kept {n_keep} samples after burn-in.\")\n\n# Calculate posterior statistics\nposterior_means = np.mean(samples, axis=0)\nposterior_sds = np.std(samples, axis=0)\nposterior_ci_lower = np.percentile(samples, 2.5, axis=0)\nposterior_ci_upper = np.percentile(samples, 97.5, axis=0)\n\n# Display Bayesian results\nprint(\"\\n=== BAYESIAN MCMC ESTIMATION RESULTS ===\")\nbayesian_results_df = pd.DataFrame({\n    'Parameter': param_names,\n    'Post_Mean': np.round(posterior_means, 4),\n    'Post_SD': np.round(posterior_sds, 4),\n    'CI_Lower': np.round(posterior_ci_lower, 4),\n    'CI_Upper': np.round(posterior_ci_upper, 4)\n})\nprint(bayesian_results_df.to_string(index=False))\n\nStarting MCMC sampling...\nIteration  1000, Acceptance rate: 0.569\nIteration  2000, Acceptance rate: 0.564\nIteration  3000, Acceptance rate: 0.575\nIteration  4000, Acceptance rate: 0.580\nIteration  5000, Acceptance rate: 0.573\nIteration  6000, Acceptance rate: 0.570\nIteration  7000, Acceptance rate: 0.570\nIteration  8000, Acceptance rate: 0.571\nIteration  9000, Acceptance rate: 0.571\nIteration 10000, Acceptance rate: 0.570\nIteration 11000, Acceptance rate: 0.570\nFinal acceptance rate: 0.570\n\nMCMC complete! Kept 10000 samples after burn-in.\n\n=== BAYESIAN MCMC ESTIMATION RESULTS ===\nParameter  Post_Mean  Post_SD  CI_Lower  CI_Upper\n  netflix     0.9360   0.1057    0.7245    1.1456\n    prime     0.4928   0.1073    0.2799    0.7098\n      ads    -0.7258   0.0891   -0.8949   -0.5474\n    price    -0.0996   0.0065   -0.1124   -0.0872\n\n\n\nimport matplotlib.pyplot as plt\n# Create trace plots and histograms for Netflix parameter (index 0)\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Netflix parameter\nparam_idx = 0\nparam_name = param_names[param_idx]\n\n# Trace plot\naxes[0, 0].plot(samples[:, param_idx])\naxes[0, 0].set_title(f'Trace Plot: {param_name.title()}')\naxes[0, 0].set_xlabel('Iteration')\naxes[0, 0].set_ylabel('Parameter Value')\naxes[0, 0].axhline(y=1.0, color='red', linestyle='--', label='True Value')\naxes[0, 0].legend()\n\n# Histogram\naxes[0, 1].hist(samples[:, param_idx], bins=50, density=True, alpha=0.7)\naxes[0, 1].axvline(x=1.0, color='red', linestyle='--', label='True Value')\naxes[0, 1].axvline(x=posterior_means[param_idx], color='blue', linestyle='-', label='Posterior Mean')\naxes[0, 1].set_title(f'Posterior Distribution: {param_name.title()}')\naxes[0, 1].set_xlabel('Parameter Value')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend()\n\n# Price parameter (more interesting because it's on different scale)\nparam_idx = 3\nparam_name = param_names[param_idx]\n\n# Trace plot\naxes[1, 0].plot(samples[:, param_idx])\naxes[1, 0].set_title(f'Trace Plot: {param_name.title()}')\naxes[1, 0].set_xlabel('Iteration')\naxes[1, 0].set_ylabel('Parameter Value')\naxes[1, 0].axhline(y=-0.1, color='red', linestyle='--', label='True Value')\naxes[1, 0].legend()\n\n# Histogram\naxes[1, 1].hist(samples[:, param_idx], bins=50, density=True, alpha=0.7)\naxes[1, 1].axvline(x=-0.1, color='red', linestyle='--', label='True Value')\naxes[1, 1].axvline(x=posterior_means[param_idx], color='blue', linestyle='-', label='Posterior Mean')\naxes[1, 1].set_title(f'Posterior Distribution: {param_name.title()}')\naxes[1, 1].set_xlabel('Parameter Value')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project3/index.html#discussion",
    "href": "blog/project3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data.\n\nprint(\"\\n ===COMPARISON OF MLE AND BAYESIAN RESULTS===\")\ncomparison_df = pd.DataFrame({\n    'Parameter': param_names,\n    'True_Value': [1.0, 0.5, -0.8, -0.1],\n    'MLE_Estimate': np.round(beta_mle, 4),\n    'MLE_SE': np.round(se_mle, 4),\n    'Bayes_Mean': np.round(posterior_means, 4),\n    'Bayes_SD': np.round(posterior_sds, 4)\n})\nprint(comparison_df.to_string(index=False))\n\nprint(\"\\n===DISCUSSION POINTS===\")\nprint(\"1.\\n ===Parameter Interpretation:===\")\nprint(f\"   - Netflix coefficient ({beta_mle[0]:.3f}) &gt; Prime coefficient ({beta_mle[1]:.3f})\")\nprint(\"     This suggests consumers prefer Netflix over Amazon Prime, with Hulu as the reference.\")\nprint(\"   - Negative price coefficient suggests consumers prefer lower prices (as expected).\")\nprint(\"   - Negative ads coefficient suggests consumers prefer ad-free content.\")\n\nprint(f\"\\n ===2. Model Performance:===\")\nprint(f\"   - Both MLE and Bayesian approaches recover parameters close to true values\")\nprint(f\"   - MCMC acceptance rate: {acceptance_rate:.3f} (reasonable for this problem)\")\nprint(f\"   - Standard errors from both methods are quite similar\")\n\nprint(f\"\\n===3. For hierarchical/multi-level models:===\")\nprint(\"   - Would need to add individual-level random effects: β_i = β + u_i\")\nprint(\"   - This requires modeling the distribution of u_i (typically multivariate normal)\")\nprint(\"   - Estimation becomes more complex, often requiring specialized software\")\nprint(\"   - Allows for heterogeneity in preferences across consumers\")\n\n\n ===COMPARISON OF MLE AND BAYESIAN RESULTS===\nParameter  True_Value  MLE_Estimate  MLE_SE  Bayes_Mean  Bayes_SD\n  netflix         1.0        0.9412  0.1110      0.9360    0.1057\n    prime         0.5        0.5016  0.1111      0.4928    0.1073\n      ads        -0.8       -0.7320  0.0878     -0.7258    0.0891\n    price        -0.1       -0.0995  0.0063     -0.0996    0.0065\n\n===DISCUSSION POINTS===\n1.\n ===Parameter Interpretation:===\n   - Netflix coefficient (0.941) &gt; Prime coefficient (0.502)\n     This suggests consumers prefer Netflix over Amazon Prime, with Hulu as the reference.\n   - Negative price coefficient suggests consumers prefer lower prices (as expected).\n   - Negative ads coefficient suggests consumers prefer ad-free content.\n\n ===2. Model Performance:===\n   - Both MLE and Bayesian approaches recover parameters close to true values\n   - MCMC acceptance rate: 0.570 (reasonable for this problem)\n   - Standard errors from both methods are quite similar\n\n===3. For hierarchical/multi-level models:===\n   - Would need to add individual-level random effects: β_i = β + u_i\n   - This requires modeling the distribution of u_i (typically multivariate normal)\n   - Estimation becomes more complex, often requiring specialized software\n   - Allows for heterogeneity in preferences across consumers"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eleanor Amidei",
    "section": "",
    "text": "I’m an experienced customer operations professional currently pursuing a Master’s in Business Analytics at UC San Diego. After several years working closely with customers and internal teams in high-growth startups, I saw firsthand how valuable data can be in improving processes, identifying opportunities, and supporting smarter decision-making. I returned to school to build a stronger foundation in analytics, focusing on the tools and techniques that turn customer and business data into meaningful and actionable insights. With a blend of practical experience and analytical training, I’m looking to contribute to data-driven teams in customer strategy, revenue operations, or sales analytics."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Eleanor Amidei",
    "section": "",
    "text": "I’m an experienced customer operations professional currently pursuing a Master’s in Business Analytics at UC San Diego. After several years working closely with customers and internal teams in high-growth startups, I saw firsthand how valuable data can be in improving processes, identifying opportunities, and supporting smarter decision-making. I returned to school to build a stronger foundation in analytics, focusing on the tools and techniques that turn customer and business data into meaningful and actionable insights. With a blend of practical experience and analytical training, I’m looking to contribute to data-driven teams in customer strategy, revenue operations, or sales analytics."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Eleanor Amidei",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA\nM.S. in Business Analytics | June 2025\nUniversity of California, Berkeley | Berkeley, CA\nB.A. in Media Studies, Minor in Public Policy | May 2018"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "About",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "resume/hw1_questions.html",
    "href": "resume/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "resume/hw1_questions.html#introduction",
    "href": "resume/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "resume/hw1_questions.html#data",
    "href": "resume/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "resume/hw1_questions.html#experimental-results",
    "href": "resume/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "resume/hw1_questions.html#simulation-experiment",
    "href": "resume/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "In their 2007 study published in the American Economic Review, economists Dean Karlan (Yale University) and John List (University of Chicago) conducted a large-scale field experiment to evaluate the impact of different types of fundraising appeals on charitable giving. The experiment was designed to test whether certain behavioral economic principles—specifically, those involving matching and challenge grants—could significantly influence donor behavior.\nThe researchers partnered with a nonprofit organization to send out 50,000 fundraising letters to potential donors. These individuals were randomly assigned to receive one of three types of solicitation letters:\nStandard Letter: This control condition contained a straightforward appeal for donations, with no mention of matching or challenge grants.\nMatching Grant Letter: This version of the letter informed potential donors that their contributions would be matched dollar-for-dollar by a large donor, effectively doubling the impact of each gift. The idea was to invoke a sense of increased efficacy and urgency.\nChallenge Grant Letter: In this version, the letter stated that a large donor had already pledged a significant amount of funding, contingent on the organization’s ability to raise additional funds from other donors. This framed the recipient’s contribution as necessary to “meet the challenge” and unlock previously pledged money.\nEach treatment group was randomized to ensure that differences in response could be causally attributed to the content of the letter. The researchers then tracked various outcomes, such as the likelihood of donating, the amount donated, and donor heterogeneity in response to the different appeals.\nThe key finding was that matching grants significantly increased both the likelihood of donating and the average donation amount, while challenge grants did not perform significantly better than the standard appeal. The results provided empirical support for the effectiveness of matching mechanisms in charitable fundraising and have since influenced both economic theory and practical strategies used by nonprofit organizations.\n\n\n\nimport pandas as pd\nimport numpy as np\n\nkarlan_data = pd.read_stata('karlan_list_2007.dta')\nprint(karlan_data.shape)\nprint(karlan_data.columns)\nprint(karlan_data.isnull().sum())\nprint(karlan_data.describe(include='all'))\n\nprint(karlan_data['treatment'].value_counts(normalize=True)) #treatment proportion\nprint(karlan_data['gave'].value_counts(normalize=True))  #donation rate \nprint(karlan_data['amount'].mean()) \nprint(karlan_data.dtypes)  \n\n#capping preview for notebook\npd.set_option('display.max_columns', 10)\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n(50083, 51)\nIndex(['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25',\n       'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1',\n       'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra',\n       'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple',\n       'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse',\n       'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush',\n       'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack',\n       'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba',\n       'pop_propurban'],\n      dtype='object')\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168561      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378115     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258654  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\ntreatment\n1    0.666813\n0    0.333187\nName: proportion, dtype: float64\ngave\n0    0.979354\n1    0.020646\nName: proportion, dtype: float64\n0.91569394\ntreatment                 int8\ncontrol                   int8\nratio                 category\nratio2                    int8\nratio3                    int8\nsize                  category\nsize25                    int8\nsize50                    int8\nsize100                   int8\nsizeno                    int8\nask                   category\naskd1                     int8\naskd2                     int8\naskd3                     int8\nask1                     int16\nask2                     int16\nask3                     int16\namount                 float32\ngave                      int8\namountchange           float32\nhpa                    float32\nltmedmra                  int8\nfreq                     int16\nyears                  float64\nyear5                     int8\nmrm2                   float64\ndormant                   int8\nfemale                 float64\ncouple                 float64\nstate50one                int8\nnonlit                 float64\ncases                  float64\nstatecnt               float32\nstateresponse          float32\nstateresponset         float32\nstateresponsec         float32\nstateresponsetminc     float32\nperbush                float32\nclose25                float64\nred0                   float64\nblue0                  float64\nredcty                 float64\nbluecty                float64\npwhite                 float32\npblack                 float32\npage18_39              float32\nave_hh_sz              float32\nmedian_hhincome        float64\npowner                 float32\npsch_atlstba           float32\npop_propurban          float32\ndtype: object\n\n\n\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWhen doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._\nI will be testing the following variables to ensure that the treatment and control groups are statistically similar: - mrm2: The number of months since the last donation. - bluecty : If the potential donor lives in a blue county. - freq: The number of prior donations. - female: The gender to the donor.\n\n\n\nfrom scipy import stats\n\n# mrm2\n# groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['mrm2'].dropna()\n\n\n# Manual calculation of t-statistic\n\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Degrees of freedom (Welch's approximation)\ndf = (var1/n1 + var2/n2)**2 / ((var1**2)/((n1**2)*(n1 - 1)) + (var2**2)/((n2**2)*(n2 - 1)))\n\n# Two-tailed p-value\nfrom scipy.stats import t\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\n\n\nprint(\"Manual p-value:\", p_value)\n\nManual t-statistic: 0.1195315522817725\nManual p-value: 0.9048549631450831\n\n\nWith a t-statistic of .12, which is not more extreme than 1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level.\nAdditionally, when we run this t-statistic through a pre-built program, we find the p_value is actually .905, which much higher than the .05 threshold we would need to reject the null hypothesis.\n\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"mrm2\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : mrm2\nExplanatory variables: control\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       13.012      0.066  196.815  &lt; .001  ***\ncontrol         -0.014      0.115   -0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\nSum of squares:\n\n                df         SS\nRegression       1          2\nError       50,080  7,309,835\nTotal       50,081  7,309,837\n\nRoot Mean Square Error (RMSE):\n12.081\n\n\nThe positive t-statistic is minimal but indicates the treatment group has a slightly higher number of months since last donation. The p-value is well above 0.05, indicating that this difference is not statistically significant at the 95% confidence level. These are the same results we saw with our manual t-test calculation.\n\n\n\n\n#bluecty - donor county is blue\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['bluecty'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['bluecty'].dropna()\n\n#t-test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\nManual t-statistic: -0.8534850504224853\n\n\nThe t-statistic is -0.85, which is not more extreme than -1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level for the blue county variable.\n\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"bluecty\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : bluecty\nExplanatory variables: control\nNull hyp.: the effect of x on bluecty is zero\nAlt. hyp.: the effect of x on bluecty is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.487      0.003  177.974  &lt; .001  ***\ncontrol          0.004      0.005    0.854   0.393     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.729 df(1, 49976), p.value 0.393\nNr obs: 49,978 (105 obs. dropped)\n\nSum of squares:\n\n                df      SS\nRegression       1       0\nError       49,976  12,487\nTotal       49,977  12,488\n\nRoot Mean Square Error (RMSE):\n0.5\n\n\nWe see the same t-statistic here, and the p-value is .396, which is well above the .05 threshold we would need to reject the null hypothesis.\n\n\n\n\n#freq - the number of prior donations\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['freq'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['freq'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"freq\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nManual t-statistic: -0.11084502380904246\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : freq\nExplanatory variables: control\nNull hyp.: the effect of x on freq is zero\nAlt. hyp.: the effect of x on freq is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        8.035      0.062  128.871  &lt; .001  ***\ncontrol          0.012      0.108    0.111   0.912     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\n\nSum of squares:\n\n                df         SS\nRegression       1          1\nError       50,081  6,502,323\nTotal       50,082  6,502,325\n\nRoot Mean Square Error (RMSE):\n11.394\n\n\nThere is no significant difference between the treatment and control groups in terms of the number of prior donations. The t-statistic is very close to zero, and the p-value is well above 0.05, indicating no statistically significant difference at the 95% confidence level. This is consistent across both the t-test and linear regression.\n\n\n\n\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['female'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['female'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"female\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nManual t-statistic: -1.7535132542519636\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : female\nExplanatory variables: control\nNull hyp.: the effect of x on female is zero\nAlt. hyp.: the effect of x on female is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.275      0.002  110.987  &lt; .001  ***\ncontrol          0.008      0.004    1.758   0.079    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.092 df(1, 48970), p.value 0.079\nNr obs: 48,972 (1,111 obs. dropped)\n\nSum of squares:\n\n                df     SS\nRegression       1      0\nError       48,970  9,821\nTotal       48,971  9,822\n\nRoot Mean Square Error (RMSE):\n0.448\n\n\nHere the t-statistic generated from the manual test is close to -1.96, but still less extreme at -1.75. This means we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level.\nWhen we run this through a pre-built program, we find the p-value is .08, which is above the .05 threshold but below the .10 threshold.\nThe difference between the percentage of female potential donors is marginally different between the two groups, this difference is not significant at the 95% confidence level. This is true in both the t-test and the linear regression. This variable however does have a significant difference at the 90% confidence level, so it would be worth noting.\n\n\n\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ndonated_treatment = karlan_data[karlan_data['treatment'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_treatment*100,2)}% of treatment group donated\")\ndonated_control = karlan_data[karlan_data['control'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_control*100,2)}% of control group donated\")\n\nplt.figure(figsize=(8, 6))\nplt.bar(['Treatment', 'Control'], [donated_treatment, donated_control], color=['green', 'pink'])\nplt.title('Donation Rates by Group')\nplt.ylabel('Donation Rate')\n\n2.2% of treatment group donated\n1.79% of control group donated\n\n\nText(0, 0.5, 'Donation Rate')\n\n\n\n\n\n\n\n\n\n\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['gave']\ncontrol_group = karlan_data[karlan_data['control'] == 1]['gave']\n\n# t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"gave\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\n#probit regression\nimport statsmodels.api as sm\n\nX = karlan_data['treatment']\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\nManual t-statistic: 3.2094621908279835\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : gave\nExplanatory variables: control\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.022      0.001   28.326  &lt; .001  ***\ncontrol         -0.004      0.001   -3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\nSum of squares:\n\n                df     SS\nRegression       1      0\nError       50,081  1,012\nTotal       50,082  1,012\n\nRoot Mean Square Error (RMSE):\n0.142\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 28 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        19:44:11   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThere difference between the treatment and control group’s response rate is significant at the 95% confidence level (the t-value is more extreme than 1.96 for a two-tailed test). The treatment group has a higher response rate than the control group, which suggests that the matching grant appeal is effective in increasing the likelihood of making a donation. This finding aligns with the hypothesis that matching grants can enhance donor motivation and engagement.\n\n\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n1:1 vs 2:1 match ratio on response rate\n\n# t-test comparing 1:1 match rate and 2:1 match rate\n\n#groups\nmatch_1_1 = karlan_data[karlan_data['ratio'] == 1]['gave']\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_1_1), len(match_2_1)\nmean1, mean2 = np.mean(match_1_1), np.mean(match_2_1)\nvar1, var2 = np.var(match_1_1, ddof=1), np.var(match_2_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\n\nprint(\"Manual t-statistic:\", t_manual)\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n\nManual t-statistic: -0.965048975142932\nManual p-value: 0.33452727037590346\n\n\n2:1 vs 3:1 match ratio on response rate\n\n# t-test comparing 2:1 match rate and 3:1 match rate\n#groups\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\nmatch_3_1 = karlan_data[karlan_data['ratio3'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_2_1), len(match_3_1)\nmean1, mean2 = np.mean(match_2_1), np.mean(match_3_1)\nvar1, var2 = np.var(match_2_1, ddof=1), np.var(match_3_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n\nManual t-statistic: -0.05011581369764474\nManual p-value: 0.9600303977894389\n\n\nThe negative t-statistic indicates that the 2:1 match ratio has a higher mean that the 1:1 ratio, however this difference is not significant at the 95% confidence level. The p-value is .34, which is well above the .05 threshold we would need to reject the null hypothesis.\nThe same applies to the 3:1 match ratio, with a p-value of .96 and a smaller difference between the two groups.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\nkarlan_data[\"ratio1\"] = (karlan_data[\"ratio\"] == 1).astype(int)\n\nX = karlan_data[['ratio1','ratio2','ratio3']]\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100430\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50079\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 28 May 2025   Pseudo R-squ.:                0.001108\nTime:                        19:44:11   Log-Likelihood:                -5029.8\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                   0.01091\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\nratio1         0.0616      0.036      1.726      0.084      -0.008       0.132\nratio2         0.0980      0.035      2.792      0.005       0.029       0.167\nratio3         0.0998      0.035      2.847      0.004       0.031       0.169\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0031      0.002      1.724      0.085      -0.000       0.007\nratio2         0.0049      0.002      2.786      0.005       0.001       0.008\nratio3         0.0050      0.002      2.841      0.004       0.002       0.008\n==============================================================================\n\n\n1:1 ratio shows a coefficient 0.062, meaning the 1:1 ratio is less likely to lead to a donation than the other ratio’s but a match should still increase the probability for a give. The 2:1 ratio has an odds ratio of 0.0980, meaning it is slightly more likely to lead to a donation than the control group. The 3:1 ratio has an odds ratio of 0.0998, meaning it is slightly more likely to lead to a donation than the 2:1 ratio. The p_values indicate that the results for ratio 2 and ratio 3 are statistically significant at the 95% confidence level, while the results for ratio 1 are not.\nCalculating the response rate differences between match ratios:\n\ncalculated directly from the data\n\n\n#means\nmean_1_1 = karlan_data[karlan_data[\"ratio\"] == 1][\"gave\"].mean()\nmean_2_1 = karlan_data[karlan_data[\"ratio\"] == 2][\"gave\"].mean()\nmean_3_1 = karlan_data[karlan_data[\"ratio\"] == 3][\"gave\"].mean()\n\n#differences\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\nprint(\"2:1 vs 1:1:\", diff_2_1_vs_1_1)\nprint(\"3:1 vs 2:1:\", diff_3_1_vs_2_1)\n\n2:1 vs 1:1: 0.0018842510217149944\n3:1 vs 2:1: 0.00010002398025293902\n\n\n\ncalculated from the differences in coefficients\n\n::: {#b6b6346d .cell execution_count=14} ``` {.python .cell-code} coef_1_1= result.params[‘ratio1’] coef_2_1= result.params[‘ratio2’] coef_3_1= result.params[‘ratio3’]\n#Difference in effects diff_2_1_vs_1_1= coef_2_1 - coef_1_1 diff_3_1_vs_2_1= coef_3_1 - coef_2_1 print(“Regression-basedeffect of 3:1 vs 2:1 match:”, diff_3_1_vs_2_1) print(“Regression-basedeffect of 2:1 vs 1:1 match:”, diff_2_1_vs_1_1)\nmfx= result.get_margeff() print(mfx.summary()) ```\n::: {.cell-output .cell-output-stdout} Regression-basedeffect of 3:1 vs 2:1 match: 0.0018572014854180002  Regression-basedeffect of 2:1 vs 1:1 match: 0.03634986488779221         Probit Marginal Effects         =====================================  Dep. Variable:                   gave  Method:                          dydx  At:                           overall  ==============================================================================                  dy/dx    std err          z      P&gt;|z|      [0.025      0.975]  ------------------------------------------------------------------------------  ratio1         0.0031      0.002      1.724      0.085      -0.000       0.007  ratio2         0.0049      0.002      2.786      0.005       0.001       0.008  ratio3         0.0050      0.002      2.841      0.004       0.002       0.008  ============================================================================== ::: :::\nBoth raw and model-based results suggest that increasing the match ratio from 1:1 to 2:1 significantly improves donation rates. However, increasing it further to 3:1 provides almost no additional benefit. These findings support the paper’s interpretation that higher match ratios can increase giving, but they also highlight diminishing returns at higher match levels.\nThe p-values for ratio1 are significant at the 90% level but the results for ratio2 and ratio3 are significant at the 95% level..\nBased on the probit model, the 3:1 match leads to a slightly higher (0.0018) latent index score for donation compared to the 2:1 match, holding everything else constant.\n\n\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution (amount of donation).\n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nIn the regression above, we learn that treatment effect has a slightly significant (not at the 95% confidence level and small) effect on size of donation.\n\nkarlan_donations = karlan_data[karlan_data['gave'] ==1 ]\nreg = rsm.model.regress({\"karlan\": karlan_donations}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nWhen only including donors, the coefficient of the regression shows us the treatment group donates about $1.67 less than the control group, however this result is not significant at the 95% confidence level. This does not have a causal interpretation because treatment may affect the likelihood of donating, and here we’re looking at the size of the donation conditional on donating.\n\ndonors = karlan_data[karlan_data['gave'] ==1]\n\n# Control\ncontrol_donors = donors[donors['treatment'] == 0]['amount']\nplt.hist(control_donors, bins=30, alpha=0.7, label='Control')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Control Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Treatment\ntreated_donors = donors[donors['treatment'] == 1]['amount']\nplt.hist(treated_donors, bins=30, alpha=0.7, label='Treatment')\nplt.axvline(treated_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Treatment Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. Further suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nHere, I simulate 10,000 draws from the control and treatment groups, then calculate their differences.\n\n#Simulated Numbers\nnp.random.seed(1)  # For reproducibilit\nsim_control = np.random.choice(control_donors, size=10000, replace=True)\nsim_treated = np.random.choice(treated_donors, size=10000, replace=True)\ndiff=sim_treated-sim_control\n\n#Cumulative Average\ncumulative_avg = np.cumsum(diff) / np.arange(1, len(diff) + 1)\n\n#True Difference in means\nTrue_Diff = treated_donors.mean() - control_donors.mean()\n\nplt.plot(cumulative_avg, label='Cumulative Average')\nplt.axhline(True_Diff, color='red', linestyle='dashed', linewidth=2, label='True Difference')\nplt.title('Cumulative Average of Differences')\nplt.xlabel('Number of Samples')\nplt.ylabel('Cumulative Average')\nplt.legend()\n\n\n\n\n\n\n\n\nBy plotting the cumulative average, we can see it approaches the true difference in means. As we increase the number of samples, the variation stabilizes. The cumulative average converges to the true difference in means. This demonstrates the Law of Large Numbers, as the sample mean approaches the population mean as the sample size increases.\n\n\n\nBelow are 4 histograms with the difference between the control and treatment group in samples sizes of 50,100,150,and 200. We then repeat the sampling 1000x to see the averages\n\np_control = 0.018\np_treatment = 0.022\n\n# parameters\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\ntrue_diff = p_treatment - p_control\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\n# Simulate and plot \nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_diffs.append(avg_diff)\n\n    axs[i].hist(avg_diffs, bins=30, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n    axs[i].axvline(x=true_diff, color='red', linestyle='--', label='True Difference (0.004)')\n    axs[i].axvline(x=0, color='black', linestyle=':', label='Zero')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg Treatment - Control\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\n# layout\nplt.suptitle(\"CLT: Distribution of Average Differences by Sample Size\", fontsize=14)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms above illustrate how the distribution of average differences in donation rates between treatment and control groups evolves as sample size increases. This simulation confirms the Central Limit Theorem (CLT).\nAt n = 50, the distribution is wide and relatively flat, and zero lies close to the center. This shows that with small sample sizes, we often can’t distinguish signal from noise, and random variation makes the estimated treatment effect unreliable.\nAt n = 200, the distribution becomes more symmetric and bell-shaped. The true treatment effect (0.004) begins to emerge, though zero is still within the central bulk of the distribution, indicating moderate uncertainty.\nAt n = 500, the distribution tightens further, and the center of the histogram clearly shifts to the right of zero. Zero now lies toward the edge (tail) of the distribution, which suggests that the true effect is increasingly distinguishable from no effect.\nAt n = 1000, the distribution is even narrower and sharply centered near 0.004. Zero is clearly in the tail, meaning that under this sample size, the true effect is more evident.\nConclusion: As sample size increases, the sampling distribution of the average difference becomes more normal and less variable, with its mean converging to the true treatment effect. Zero moves from the center to the tails of the distribution, reinforcing that larger samples improve the precision of effect estimates and the reliability of hypothesis testing."
  },
  {
    "objectID": "blog/project1/index.html#data-exploration",
    "href": "blog/project1/index.html#data-exploration",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nkarlan_data = pd.read_stata('karlan_list_2007.dta')\nprint(karlan_data.shape)\nprint(karlan_data.columns)\nprint(karlan_data.isnull().sum())\nprint(karlan_data.describe(include='all'))\n\nprint(karlan_data['treatment'].value_counts(normalize=True)) #treatment proportion\nprint(karlan_data['gave'].value_counts(normalize=True))  #donation rate \nprint(karlan_data['amount'].mean()) \nprint(karlan_data.dtypes)  \n\n#capping preview for notebook\npd.set_option('display.max_columns', 10)\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n(50083, 51)\nIndex(['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25',\n       'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1',\n       'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra',\n       'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple',\n       'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse',\n       'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush',\n       'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack',\n       'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba',\n       'pop_propurban'],\n      dtype='object')\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168561      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378115     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258654  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\ntreatment\n1    0.666813\n0    0.333187\nName: proportion, dtype: float64\ngave\n0    0.979354\n1    0.020646\nName: proportion, dtype: float64\n0.91569394\ntreatment                 int8\ncontrol                   int8\nratio                 category\nratio2                    int8\nratio3                    int8\nsize                  category\nsize25                    int8\nsize50                    int8\nsize100                   int8\nsizeno                    int8\nask                   category\naskd1                     int8\naskd2                     int8\naskd3                     int8\nask1                     int16\nask2                     int16\nask3                     int16\namount                 float32\ngave                      int8\namountchange           float32\nhpa                    float32\nltmedmra                  int8\nfreq                     int16\nyears                  float64\nyear5                     int8\nmrm2                   float64\ndormant                   int8\nfemale                 float64\ncouple                 float64\nstate50one                int8\nnonlit                 float64\ncases                  float64\nstatecnt               float32\nstateresponse          float32\nstateresponset         float32\nstateresponsec         float32\nstateresponsetminc     float32\nperbush                float32\nclose25                float64\nred0                   float64\nblue0                  float64\nredcty                 float64\nbluecty                float64\npwhite                 float32\npblack                 float32\npage18_39              float32\nave_hh_sz              float32\nmedian_hhincome        float64\npowner                 float32\npsch_atlstba           float32\npop_propurban          float32\ndtype: object"
  },
  {
    "objectID": "blog/project1/index.html#balance-test",
    "href": "blog/project1/index.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nWhen doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._\nI will be testing the following variables to ensure that the treatment and control groups are statistically similar: - mrm2: The number of months since the last donation. - bluecty : If the potential donor lives in a blue county. - freq: The number of prior donations. - female: The gender to the donor.\n\n\n\nfrom scipy import stats\n\n# mrm2\n# groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['mrm2'].dropna()\n\n\n# Manual calculation of t-statistic\n\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Degrees of freedom (Welch's approximation)\ndf = (var1/n1 + var2/n2)**2 / ((var1**2)/((n1**2)*(n1 - 1)) + (var2**2)/((n2**2)*(n2 - 1)))\n\n# Two-tailed p-value\nfrom scipy.stats import t\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\n\n\nprint(\"Manual p-value:\", p_value)\n\nManual t-statistic: 0.1195315522817725\nManual p-value: 0.9048549631450831\n\n\nWith a t-statistic of .12, which is not more extreme than 1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level.\nAdditionally, when we run this t-statistic through a pre-built program, we find the p_value is actually .905, which much higher than the .05 threshold we would need to reject the null hypothesis.\n\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"mrm2\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : mrm2\nExplanatory variables: control\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       13.012      0.066  196.815  &lt; .001  ***\ncontrol         -0.014      0.115   -0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082 (1 obs. dropped)\n\nSum of squares:\n\n                df         SS\nRegression       1          2\nError       50,080  7,309,835\nTotal       50,081  7,309,837\n\nRoot Mean Square Error (RMSE):\n12.081\n\n\nThe positive t-statistic is minimal but indicates the treatment group has a slightly higher number of months since last donation. The p-value is well above 0.05, indicating that this difference is not statistically significant at the 95% confidence level. These are the same results we saw with our manual t-test calculation.\n\n\n\n\n#bluecty - donor county is blue\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['bluecty'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['bluecty'].dropna()\n\n#t-test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\nManual t-statistic: -0.8534850504224853\n\n\nThe t-statistic is -0.85, which is not more extreme than -1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level for the blue county variable.\n\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"bluecty\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : bluecty\nExplanatory variables: control\nNull hyp.: the effect of x on bluecty is zero\nAlt. hyp.: the effect of x on bluecty is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.487      0.003  177.974  &lt; .001  ***\ncontrol          0.004      0.005    0.854   0.393     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.729 df(1, 49976), p.value 0.393\nNr obs: 49,978 (105 obs. dropped)\n\nSum of squares:\n\n                df      SS\nRegression       1       0\nError       49,976  12,487\nTotal       49,977  12,488\n\nRoot Mean Square Error (RMSE):\n0.5\n\n\nWe see the same t-statistic here, and the p-value is .396, which is well above the .05 threshold we would need to reject the null hypothesis.\n\n\n\n\n#freq - the number of prior donations\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['freq'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['freq'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"freq\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nManual t-statistic: -0.11084502380904246\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : freq\nExplanatory variables: control\nNull hyp.: the effect of x on freq is zero\nAlt. hyp.: the effect of x on freq is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        8.035      0.062  128.871  &lt; .001  ***\ncontrol          0.012      0.108    0.111   0.912     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\n\nSum of squares:\n\n                df         SS\nRegression       1          1\nError       50,081  6,502,323\nTotal       50,082  6,502,325\n\nRoot Mean Square Error (RMSE):\n11.394\n\n\nThere is no significant difference between the treatment and control groups in terms of the number of prior donations. The t-statistic is very close to zero, and the p-value is well above 0.05, indicating no statistically significant difference at the 95% confidence level. This is consistent across both the t-test and linear regression.\n\n\n\n\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['female'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['female'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"female\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\nManual t-statistic: -1.7535132542519636\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : female\nExplanatory variables: control\nNull hyp.: the effect of x on female is zero\nAlt. hyp.: the effect of x on female is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.275      0.002  110.987  &lt; .001  ***\ncontrol          0.008      0.004    1.758   0.079    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.092 df(1, 48970), p.value 0.079\nNr obs: 48,972 (1,111 obs. dropped)\n\nSum of squares:\n\n                df     SS\nRegression       1      0\nError       48,970  9,821\nTotal       48,971  9,822\n\nRoot Mean Square Error (RMSE):\n0.448\n\n\nHere the t-statistic generated from the manual test is close to -1.96, but still less extreme at -1.75. This means we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level.\nWhen we run this through a pre-built program, we find the p-value is .08, which is above the .05 threshold but below the .10 threshold.\nThe difference between the percentage of female potential donors is marginally different between the two groups, this difference is not significant at the 95% confidence level. This is true in both the t-test and the linear regression. This variable however does have a significant difference at the 90% confidence level, so it would be worth noting."
  },
  {
    "objectID": "blog/project1/index.html#charitable-contribution-made",
    "href": "blog/project1/index.html#charitable-contribution-made",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "First, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ndonated_treatment = karlan_data[karlan_data['treatment'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_treatment*100,2)}% of treatment group donated\")\ndonated_control = karlan_data[karlan_data['control'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_control*100,2)}% of control group donated\")\n\nplt.figure(figsize=(8, 6))\nplt.bar(['Treatment', 'Control'], [donated_treatment, donated_control], color=['green', 'pink'])\nplt.title('Donation Rates by Group')\nplt.ylabel('Donation Rate')\n\n2.2% of treatment group donated\n1.79% of control group donated\n\n\nText(0, 0.5, 'Donation Rate')\n\n\n\n\n\n\n\n\n\n\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['gave']\ncontrol_group = karlan_data[karlan_data['control'] == 1]['gave']\n\n# t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"gave\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\n#probit regression\nimport statsmodels.api as sm\n\nX = karlan_data['treatment']\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\nManual t-statistic: 3.2094621908279835\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : gave\nExplanatory variables: control\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.022      0.001   28.326  &lt; .001  ***\ncontrol         -0.004      0.001   -3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\nSum of squares:\n\n                df     SS\nRegression       1      0\nError       50,081  1,012\nTotal       50,082  1,012\n\nRoot Mean Square Error (RMSE):\n0.142\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 28 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        19:44:11   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThere difference between the treatment and control group’s response rate is significant at the 95% confidence level (the t-value is more extreme than 1.96 for a two-tailed test). The treatment group has a higher response rate than the control group, which suggests that the matching grant appeal is effective in increasing the likelihood of making a donation. This finding aligns with the hypothesis that matching grants can enhance donor motivation and engagement."
  },
  {
    "objectID": "blog/project1/index.html#differences-between-match-rates",
    "href": "blog/project1/index.html#differences-between-match-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Next, I assess the effectiveness of different sizes of matched donations on the response rate.\n1:1 vs 2:1 match ratio on response rate\n\n# t-test comparing 1:1 match rate and 2:1 match rate\n\n#groups\nmatch_1_1 = karlan_data[karlan_data['ratio'] == 1]['gave']\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_1_1), len(match_2_1)\nmean1, mean2 = np.mean(match_1_1), np.mean(match_2_1)\nvar1, var2 = np.var(match_1_1, ddof=1), np.var(match_2_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\n\nprint(\"Manual t-statistic:\", t_manual)\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n\nManual t-statistic: -0.965048975142932\nManual p-value: 0.33452727037590346\n\n\n2:1 vs 3:1 match ratio on response rate\n\n# t-test comparing 2:1 match rate and 3:1 match rate\n#groups\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\nmatch_3_1 = karlan_data[karlan_data['ratio3'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_2_1), len(match_3_1)\nmean1, mean2 = np.mean(match_2_1), np.mean(match_3_1)\nvar1, var2 = np.var(match_2_1, ddof=1), np.var(match_3_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n\nManual t-statistic: -0.05011581369764474\nManual p-value: 0.9600303977894389\n\n\nThe negative t-statistic indicates that the 2:1 match ratio has a higher mean that the 1:1 ratio, however this difference is not significant at the 95% confidence level. The p-value is .34, which is well above the .05 threshold we would need to reject the null hypothesis.\nThe same applies to the 3:1 match ratio, with a p-value of .96 and a smaller difference between the two groups.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\nkarlan_data[\"ratio1\"] = (karlan_data[\"ratio\"] == 1).astype(int)\n\nX = karlan_data[['ratio1','ratio2','ratio3']]\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100430\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50079\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 28 May 2025   Pseudo R-squ.:                0.001108\nTime:                        19:44:11   Log-Likelihood:                -5029.8\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                   0.01091\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\nratio1         0.0616      0.036      1.726      0.084      -0.008       0.132\nratio2         0.0980      0.035      2.792      0.005       0.029       0.167\nratio3         0.0998      0.035      2.847      0.004       0.031       0.169\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0031      0.002      1.724      0.085      -0.000       0.007\nratio2         0.0049      0.002      2.786      0.005       0.001       0.008\nratio3         0.0050      0.002      2.841      0.004       0.002       0.008\n==============================================================================\n\n\n1:1 ratio shows a coefficient 0.062, meaning the 1:1 ratio is less likely to lead to a donation than the other ratio’s but a match should still increase the probability for a give. The 2:1 ratio has an odds ratio of 0.0980, meaning it is slightly more likely to lead to a donation than the control group. The 3:1 ratio has an odds ratio of 0.0998, meaning it is slightly more likely to lead to a donation than the 2:1 ratio. The p_values indicate that the results for ratio 2 and ratio 3 are statistically significant at the 95% confidence level, while the results for ratio 1 are not.\nCalculating the response rate differences between match ratios:\n\ncalculated directly from the data\n\n\n#means\nmean_1_1 = karlan_data[karlan_data[\"ratio\"] == 1][\"gave\"].mean()\nmean_2_1 = karlan_data[karlan_data[\"ratio\"] == 2][\"gave\"].mean()\nmean_3_1 = karlan_data[karlan_data[\"ratio\"] == 3][\"gave\"].mean()\n\n#differences\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\nprint(\"2:1 vs 1:1:\", diff_2_1_vs_1_1)\nprint(\"3:1 vs 2:1:\", diff_3_1_vs_2_1)\n\n2:1 vs 1:1: 0.0018842510217149944\n3:1 vs 2:1: 0.00010002398025293902\n\n\n\ncalculated from the differences in coefficients\n\n::: {#b6b6346d .cell execution_count=14} ``` {.python .cell-code} coef_1_1= result.params[‘ratio1’] coef_2_1= result.params[‘ratio2’] coef_3_1= result.params[‘ratio3’]\n#Difference in effects diff_2_1_vs_1_1= coef_2_1 - coef_1_1 diff_3_1_vs_2_1= coef_3_1 - coef_2_1 print(“Regression-basedeffect of 3:1 vs 2:1 match:”, diff_3_1_vs_2_1) print(“Regression-basedeffect of 2:1 vs 1:1 match:”, diff_2_1_vs_1_1)\nmfx= result.get_margeff() print(mfx.summary()) ```\n::: {.cell-output .cell-output-stdout} Regression-basedeffect of 3:1 vs 2:1 match: 0.0018572014854180002  Regression-basedeffect of 2:1 vs 1:1 match: 0.03634986488779221         Probit Marginal Effects         =====================================  Dep. Variable:                   gave  Method:                          dydx  At:                           overall  ==============================================================================                  dy/dx    std err          z      P&gt;|z|      [0.025      0.975]  ------------------------------------------------------------------------------  ratio1         0.0031      0.002      1.724      0.085      -0.000       0.007  ratio2         0.0049      0.002      2.786      0.005       0.001       0.008  ratio3         0.0050      0.002      2.841      0.004       0.002       0.008  ============================================================================== ::: :::\nBoth raw and model-based results suggest that increasing the match ratio from 1:1 to 2:1 significantly improves donation rates. However, increasing it further to 3:1 provides almost no additional benefit. These findings support the paper’s interpretation that higher match ratios can increase giving, but they also highlight diminishing returns at higher match levels.\nThe p-values for ratio1 are significant at the 90% level but the results for ratio2 and ratio3 are significant at the 95% level..\nBased on the probit model, the 3:1 match leads to a slightly higher (0.0018) latent index score for donation compared to the 2:1 match, holding everything else constant."
  },
  {
    "objectID": "blog/project1/index.html#size-of-charitable-contribution",
    "href": "blog/project1/index.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution (amount of donation).\n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.813      0.067   12.063  &lt; .001  ***\ntreatment        0.154      0.083    1.861   0.063    .\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.461 df(1, 50081), p.value 0.063\nNr obs: 50,083\n\n\nIn the regression above, we learn that treatment effect has a slightly significant (not at the 95% confidence level and small) effect on size of donation.\n\nkarlan_donations = karlan_data[karlan_data['gave'] ==1 ]\nreg = rsm.model.regress({\"karlan\": karlan_donations}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n\nLinear regression (OLS)\nData                 : karlan\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nWhen only including donors, the coefficient of the regression shows us the treatment group donates about $1.67 less than the control group, however this result is not significant at the 95% confidence level. This does not have a causal interpretation because treatment may affect the likelihood of donating, and here we’re looking at the size of the donation conditional on donating.\n\ndonors = karlan_data[karlan_data['gave'] ==1]\n\n# Control\ncontrol_donors = donors[donors['treatment'] == 0]['amount']\nplt.hist(control_donors, bins=30, alpha=0.7, label='Control')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Control Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Treatment\ntreated_donors = donors[donors['treatment'] == 1]['amount']\nplt.hist(treated_donors, bins=30, alpha=0.7, label='Treatment')\nplt.axvline(treated_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Treatment Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "As a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. Further suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nHere, I simulate 10,000 draws from the control and treatment groups, then calculate their differences.\n\n#Simulated Numbers\nnp.random.seed(1)  # For reproducibilit\nsim_control = np.random.choice(control_donors, size=10000, replace=True)\nsim_treated = np.random.choice(treated_donors, size=10000, replace=True)\ndiff=sim_treated-sim_control\n\n#Cumulative Average\ncumulative_avg = np.cumsum(diff) / np.arange(1, len(diff) + 1)\n\n#True Difference in means\nTrue_Diff = treated_donors.mean() - control_donors.mean()\n\nplt.plot(cumulative_avg, label='Cumulative Average')\nplt.axhline(True_Diff, color='red', linestyle='dashed', linewidth=2, label='True Difference')\nplt.title('Cumulative Average of Differences')\nplt.xlabel('Number of Samples')\nplt.ylabel('Cumulative Average')\nplt.legend()\n\n\n\n\n\n\n\n\nBy plotting the cumulative average, we can see it approaches the true difference in means. As we increase the number of samples, the variation stabilizes. The cumulative average converges to the true difference in means. This demonstrates the Law of Large Numbers, as the sample mean approaches the population mean as the sample size increases.\n\n\n\nBelow are 4 histograms with the difference between the control and treatment group in samples sizes of 50,100,150,and 200. We then repeat the sampling 1000x to see the averages\n\np_control = 0.018\np_treatment = 0.022\n\n# parameters\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\ntrue_diff = p_treatment - p_control\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\n# Simulate and plot \nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_diffs.append(avg_diff)\n\n    axs[i].hist(avg_diffs, bins=30, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n    axs[i].axvline(x=true_diff, color='red', linestyle='--', label='True Difference (0.004)')\n    axs[i].axvline(x=0, color='black', linestyle=':', label='Zero')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg Treatment - Control\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\n# layout\nplt.suptitle(\"CLT: Distribution of Average Differences by Sample Size\", fontsize=14)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms above illustrate how the distribution of average differences in donation rates between treatment and control groups evolves as sample size increases. This simulation confirms the Central Limit Theorem (CLT).\nAt n = 50, the distribution is wide and relatively flat, and zero lies close to the center. This shows that with small sample sizes, we often can’t distinguish signal from noise, and random variation makes the estimated treatment effect unreliable.\nAt n = 200, the distribution becomes more symmetric and bell-shaped. The true treatment effect (0.004) begins to emerge, though zero is still within the central bulk of the distribution, indicating moderate uncertainty.\nAt n = 500, the distribution tightens further, and the center of the histogram clearly shifts to the right of zero. Zero now lies toward the edge (tail) of the distribution, which suggests that the true effect is increasingly distinguishable from no effect.\nAt n = 1000, the distribution is even narrower and sharply centered near 0.004. Zero is clearly in the tail, meaning that under this sample size, the true effect is more evident.\nConclusion: As sample size increases, the sampling distribution of the average difference becomes more normal and less variable, with its mean converging to the true treatment effect. Zero moves from the center to the tails of the distribution, reinforcing that larger samples improve the precision of effect estimates and the reliability of hypothesis testing."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nairbnb = pd.read_csv('airbnb.csv')\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', kde=False, bins=30, multiple='dodge')\nplt.title('Distribution of Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\nblueprinty.groupby('iscustomer')['patents'].mean()\n\nprint(blueprinty.groupby('iscustomer')['patents'].mean())\n\n# T-test \nfrom scipy.stats import ttest_ind\n\ncust = blueprinty[blueprinty['iscustomer'] == 1]['patents']\nnoncust = blueprinty[blueprinty['iscustomer'] == 0]['patents']\n\nt_stat, p_val = ttest_ind(cust, noncust, equal_var=False)\nprint(f\"T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}\")\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('patents ~ iscustomer + age + C(region)', data=blueprinty).fit()\nprint(model.summary())\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\nT-statistic: 4.87, p-value: 0.0000\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                patents   R-squared:                       0.032\nModel:                            OLS   Adj. R-squared:                  0.028\nMethod:                 Least Squares   F-statistic:                     8.214\nDate:                Wed, 28 May 2025   Prob (F-statistic):           9.17e-09\nTime:                        19:49:19   Log-Likelihood:                -3386.8\nNo. Observations:                1500   AIC:                             6788.\nDf Residuals:                    1493   BIC:                             6825.\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                  4.2775      0.274     15.606      0.000       3.740       4.815\nC(region)[T.Northeast]     0.2172      0.190      1.146      0.252      -0.155       0.589\nC(region)[T.Northwest]    -0.0480      0.230     -0.209      0.835      -0.499       0.403\nC(region)[T.South]         0.1718      0.228      0.752      0.452      -0.276       0.620\nC(region)[T.Southwest]     0.2458      0.206      1.195      0.232      -0.158       0.649\niscustomer                 0.6407      0.140      4.578      0.000       0.366       0.915\nage                       -0.0360      0.008     -4.335      0.000      -0.052      -0.020\n==============================================================================\nOmnibus:                      242.423   Durbin-Watson:                   2.034\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              448.197\nSkew:                           0.993   Prob(JB):                     4.73e-98\nKurtosis:                       4.796   Cond. No.                         178.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTest of means:\nBlueprinty customers have significantly more patents than non-customers on average (4.13 vs. 3.47), and the difference is statistically significant (p &lt; 0.001). However, this is an unadjusted comparison and may be influenced by other factors like age or region.\n\nCustomer status is assiociated with more patients:\n\niscustomer coefficient = +0.641, p &lt; 0.001\nInterpretation: Controlling for age and region, Blueprinty customers have, on average, 0.64 more patents than non-customers.\nStatistically significant at the 0.1% level → this relationship is unlikely to be due to chance.\n\nAge Has a Negative Effect:\n\nage coefficient = -0.036, p &lt; 0.001\nInterpretation: For each additional year of company age, patent count decreases slightly (by 0.036).\nThis might suggest younger companies are more innovative, or older ones already hold established portfolios.\n\nRegion Doesn’t Seem to Matter Much All region coefficients are statistically insignificant (p &gt; 0.05).\n\nNo strong evidence that region has a meaningful effect on patent count in this sample.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nimport numpy as np\nfrom scipy.special import gammaln  \nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\n# Use actual Y values from the blueprinty dataset\nY = blueprinty['patents'].values\n\n# Lambda range for plotting\nlambdas = np.linspace(0.1, 20, 200)\nlogliks = [poisson_loglikelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambdas, logliks)\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n# We minimize the negative log-likelihood\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = optimize.minimize_scalar(neg_loglik, bounds=(0.01, 50), method='bounded')\nlambda_mle = result.x\n\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y: {np.mean(Y):.4f}\")\n\n\n\n\n\n\n\n\nMLE for lambda: 3.6847\nSample mean of Y: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\nimport numpy as np\nimport pandas as pd\nimport scipy.special\nimport scipy.optimize\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Poisson\nfrom scipy import stats\n\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.array(beta)\n    Y = np.array(Y)\n    X = np.array(X)\n    \n    linear_pred = X.dot(beta)\n    \n    linear_pred = np.clip(linear_pred, -30, 30)\n    \n    lambda_i = np.exp(linear_pred)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i + 1e-10) - lambda_i - scipy.special.gammaln(Y + 1))\n    \n    return log_likelihood\n\ndef negative_poisson_loglikelihood(beta, Y, X):\n    return -poisson_loglikelihood(beta, Y, X)\n\nblueprinty['age_squared'] = blueprinty['age'].astype(float) ** 2\nregion_dummies = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)\n\nX_data = pd.DataFrame()\nX_data['age'] = blueprinty['age'].astype(float)\nX_data['age_squared'] = blueprinty['age_squared'].astype(float)\nX_data['iscustomer'] = blueprinty['iscustomer'].astype(float)\n\nfor col in region_dummies.columns:\n    X_data[col] = region_dummies[col].astype(float)\n\nX = sm.add_constant(X_data)\nY = blueprinty['patents'].astype(float)\n\nX_array = np.asarray(X)\nY_array = np.asarray(Y)\n\npoisson_model = sm.GLM(Y_array, X_array, family=Poisson())\npoisson_results = poisson_model.fit()\ninitial_beta = poisson_results.params\n\nresult = scipy.optimize.minimize(\n    negative_poisson_loglikelihood,\n    initial_beta,\n    args=(Y_array, X_array),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_mle = result.x\n\nstd_errors = poisson_results.bse\n\ncolumn_names = ['Intercept', 'Age', 'Age²']\nregion_cols = list(region_dummies.columns)\ncolumn_names.extend(region_cols)\ncolumn_names.append('Customer')\n\ncomparison_df = pd.DataFrame({\n    'Manual Coefficient': beta_mle,\n    'Statsmodels Coefficient': poisson_results.params,\n    'Std. Error': std_errors,\n    'z-value': poisson_results.params / std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(poisson_results.params / std_errors)))\n})\ncomparison_df.index = column_names[:len(beta_mle)]\n\nprint(\"Poisson Regression Results (Comparison):\")\nprint(comparison_df)\n\niscustomer_idx = list(X.columns).index('iscustomer')\ncustomer_effect = np.exp(poisson_results.params[iscustomer_idx]) - 1\nprint(f\"\\nEffect of being a Blueprinty customer: {customer_effect:.4f}\")\nprint(f\"Blueprinty customers are associated with a {customer_effect*100:.2f}% increase in patent count.\")\n\nfrom scipy.optimize import approx_fprime\n\ndef hessian(func, x, *args):\n    n = len(x)\n    h = 1e-5 \n    hessy = np.zeros((n, n))\n    \n    def grad(x, *args):\n        return approx_fprime(x, func, h, *args)\n    \n    for i in range(n):\n        x_plus = x.copy()\n        x_plus[i] += h\n        grad_plus = grad(x_plus, *args)\n        \n        grad_x = grad(x, *args)\n        \n        hessy[i] = (grad_plus - grad_x) / h\n    \n    hessy = (hessy + hessy.T) / 2\n    \n    return hessy\n\nhessy = hessian(negative_poisson_loglikelihood, beta_mle, Y_array, X_array)\n\ncov_matrix = np.linalg.inv(hessy)\nmanual_std_errors = np.sqrt(np.diag(cov_matrix))\n\nmanual_results_df = pd.DataFrame({\n    'Coefficient': beta_mle,\n    'Manual Std. Error': manual_std_errors,\n    'Statsmodels Std. Error': std_errors,\n    'z-value': beta_mle / manual_std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(beta_mle / manual_std_errors)))\n})\nmanual_results_df.index = column_names[:len(beta_mle)]\n\nprint(\"\\nPoisson Regression Results with MSE:\")\nprint(manual_results_df)\n\n         Current function value: 3258.072145\n         Iterations: 0\n         Function evaluations: 363\n         Gradient evaluations: 39\nPoisson Regression Results (Comparison):\n                  Manual Coefficient  Statsmodels Coefficient  Std. Error  \\\nIntercept                  -0.508920                -0.508920    0.183179   \nAge                         0.148619                 0.148619    0.013869   \nAge²                       -0.002970                -0.002970    0.000258   \nregion_Northeast            0.207591                 0.207591    0.030895   \nregion_Northwest            0.029170                 0.029170    0.043625   \nregion_South               -0.017575                -0.017575    0.053781   \nregion_Southwest            0.056561                 0.056561    0.052662   \nCustomer                    0.050576                 0.050576    0.047198   \n\n                    z-value       p-value  \nIntercept         -2.778269  5.464935e-03  \nAge               10.716250  0.000000e+00  \nAge²             -11.513237  0.000000e+00  \nregion_Northeast   6.719179  1.827516e-11  \nregion_Northwest   0.668647  5.037205e-01  \nregion_South      -0.326782  7.438327e-01  \nregion_Southwest   1.074036  2.828066e-01  \nCustomer           1.071568  2.839141e-01  \n\nEffect of being a Blueprinty customer: 0.2307\nBlueprinty customers are associated with a 23.07% increase in patent count.\n\nPoisson Regression Results with MSE:\n                  Coefficient  Manual Std. Error  Statsmodels Std. Error  \\\nIntercept           -0.508920           0.181517                0.183179   \nAge                  0.148619           0.013674                0.013869   \nAge²                -0.002970           0.000252                0.000258   \nregion_Northeast     0.207591           0.030895                0.030895   \nregion_Northwest     0.029170           0.043625                0.043625   \nregion_South        -0.017575           0.053779                0.053781   \nregion_Southwest     0.056561           0.052662                0.052662   \nCustomer             0.050576           0.047198                0.047198   \n\n                    z-value       p-value  \nIntercept         -2.803698  5.052015e-03  \nAge               10.868563  0.000000e+00  \nAge²             -11.773299  0.000000e+00  \nregion_Northeast   6.719176  1.827560e-11  \nregion_Northwest   0.668647  5.037205e-01  \nregion_South      -0.326789  7.438276e-01  \nregion_Southwest   1.074045  2.828024e-01  \nCustomer           1.071572  2.839121e-01  \n\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)"
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nairbnb = pd.read_csv('airbnb.csv')\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', kde=False, bins=30, multiple='dodge')\nplt.title('Distribution of Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\nblueprinty.groupby('iscustomer')['patents'].mean()\n\nprint(blueprinty.groupby('iscustomer')['patents'].mean())\n\n# T-test \nfrom scipy.stats import ttest_ind\n\ncust = blueprinty[blueprinty['iscustomer'] == 1]['patents']\nnoncust = blueprinty[blueprinty['iscustomer'] == 0]['patents']\n\nt_stat, p_val = ttest_ind(cust, noncust, equal_var=False)\nprint(f\"T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}\")\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('patents ~ iscustomer + age + C(region)', data=blueprinty).fit()\nprint(model.summary())\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\nT-statistic: 4.87, p-value: 0.0000\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                patents   R-squared:                       0.032\nModel:                            OLS   Adj. R-squared:                  0.028\nMethod:                 Least Squares   F-statistic:                     8.214\nDate:                Wed, 28 May 2025   Prob (F-statistic):           9.17e-09\nTime:                        19:49:19   Log-Likelihood:                -3386.8\nNo. Observations:                1500   AIC:                             6788.\nDf Residuals:                    1493   BIC:                             6825.\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nIntercept                  4.2775      0.274     15.606      0.000       3.740       4.815\nC(region)[T.Northeast]     0.2172      0.190      1.146      0.252      -0.155       0.589\nC(region)[T.Northwest]    -0.0480      0.230     -0.209      0.835      -0.499       0.403\nC(region)[T.South]         0.1718      0.228      0.752      0.452      -0.276       0.620\nC(region)[T.Southwest]     0.2458      0.206      1.195      0.232      -0.158       0.649\niscustomer                 0.6407      0.140      4.578      0.000       0.366       0.915\nage                       -0.0360      0.008     -4.335      0.000      -0.052      -0.020\n==============================================================================\nOmnibus:                      242.423   Durbin-Watson:                   2.034\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              448.197\nSkew:                           0.993   Prob(JB):                     4.73e-98\nKurtosis:                       4.796   Cond. No.                         178.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTest of means:\nBlueprinty customers have significantly more patents than non-customers on average (4.13 vs. 3.47), and the difference is statistically significant (p &lt; 0.001). However, this is an unadjusted comparison and may be influenced by other factors like age or region.\n\nCustomer status is assiociated with more patients:\n\niscustomer coefficient = +0.641, p &lt; 0.001\nInterpretation: Controlling for age and region, Blueprinty customers have, on average, 0.64 more patents than non-customers.\nStatistically significant at the 0.1% level → this relationship is unlikely to be due to chance.\n\nAge Has a Negative Effect:\n\nage coefficient = -0.036, p &lt; 0.001\nInterpretation: For each additional year of company age, patent count decreases slightly (by 0.036).\nThis might suggest younger companies are more innovative, or older ones already hold established portfolios.\n\nRegion Doesn’t Seem to Matter Much All region coefficients are statistically insignificant (p &gt; 0.05).\n\nNo strong evidence that region has a meaningful effect on patent count in this sample.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nimport numpy as np\nfrom scipy.special import gammaln  \nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\n# Use actual Y values from the blueprinty dataset\nY = blueprinty['patents'].values\n\n# Lambda range for plotting\nlambdas = np.linspace(0.1, 20, 200)\nlogliks = [poisson_loglikelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambdas, logliks)\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n# We minimize the negative log-likelihood\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = optimize.minimize_scalar(neg_loglik, bounds=(0.01, 50), method='bounded')\nlambda_mle = result.x\n\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y: {np.mean(Y):.4f}\")\n\n\n\n\n\n\n\n\nMLE for lambda: 3.6847\nSample mean of Y: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\nimport numpy as np\nimport pandas as pd\nimport scipy.special\nimport scipy.optimize\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Poisson\nfrom scipy import stats\n\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.array(beta)\n    Y = np.array(Y)\n    X = np.array(X)\n    \n    linear_pred = X.dot(beta)\n    \n    linear_pred = np.clip(linear_pred, -30, 30)\n    \n    lambda_i = np.exp(linear_pred)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i + 1e-10) - lambda_i - scipy.special.gammaln(Y + 1))\n    \n    return log_likelihood\n\ndef negative_poisson_loglikelihood(beta, Y, X):\n    return -poisson_loglikelihood(beta, Y, X)\n\nblueprinty['age_squared'] = blueprinty['age'].astype(float) ** 2\nregion_dummies = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)\n\nX_data = pd.DataFrame()\nX_data['age'] = blueprinty['age'].astype(float)\nX_data['age_squared'] = blueprinty['age_squared'].astype(float)\nX_data['iscustomer'] = blueprinty['iscustomer'].astype(float)\n\nfor col in region_dummies.columns:\n    X_data[col] = region_dummies[col].astype(float)\n\nX = sm.add_constant(X_data)\nY = blueprinty['patents'].astype(float)\n\nX_array = np.asarray(X)\nY_array = np.asarray(Y)\n\npoisson_model = sm.GLM(Y_array, X_array, family=Poisson())\npoisson_results = poisson_model.fit()\ninitial_beta = poisson_results.params\n\nresult = scipy.optimize.minimize(\n    negative_poisson_loglikelihood,\n    initial_beta,\n    args=(Y_array, X_array),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_mle = result.x\n\nstd_errors = poisson_results.bse\n\ncolumn_names = ['Intercept', 'Age', 'Age²']\nregion_cols = list(region_dummies.columns)\ncolumn_names.extend(region_cols)\ncolumn_names.append('Customer')\n\ncomparison_df = pd.DataFrame({\n    'Manual Coefficient': beta_mle,\n    'Statsmodels Coefficient': poisson_results.params,\n    'Std. Error': std_errors,\n    'z-value': poisson_results.params / std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(poisson_results.params / std_errors)))\n})\ncomparison_df.index = column_names[:len(beta_mle)]\n\nprint(\"Poisson Regression Results (Comparison):\")\nprint(comparison_df)\n\niscustomer_idx = list(X.columns).index('iscustomer')\ncustomer_effect = np.exp(poisson_results.params[iscustomer_idx]) - 1\nprint(f\"\\nEffect of being a Blueprinty customer: {customer_effect:.4f}\")\nprint(f\"Blueprinty customers are associated with a {customer_effect*100:.2f}% increase in patent count.\")\n\nfrom scipy.optimize import approx_fprime\n\ndef hessian(func, x, *args):\n    n = len(x)\n    h = 1e-5 \n    hessy = np.zeros((n, n))\n    \n    def grad(x, *args):\n        return approx_fprime(x, func, h, *args)\n    \n    for i in range(n):\n        x_plus = x.copy()\n        x_plus[i] += h\n        grad_plus = grad(x_plus, *args)\n        \n        grad_x = grad(x, *args)\n        \n        hessy[i] = (grad_plus - grad_x) / h\n    \n    hessy = (hessy + hessy.T) / 2\n    \n    return hessy\n\nhessy = hessian(negative_poisson_loglikelihood, beta_mle, Y_array, X_array)\n\ncov_matrix = np.linalg.inv(hessy)\nmanual_std_errors = np.sqrt(np.diag(cov_matrix))\n\nmanual_results_df = pd.DataFrame({\n    'Coefficient': beta_mle,\n    'Manual Std. Error': manual_std_errors,\n    'Statsmodels Std. Error': std_errors,\n    'z-value': beta_mle / manual_std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(beta_mle / manual_std_errors)))\n})\nmanual_results_df.index = column_names[:len(beta_mle)]\n\nprint(\"\\nPoisson Regression Results with MSE:\")\nprint(manual_results_df)\n\n         Current function value: 3258.072145\n         Iterations: 0\n         Function evaluations: 363\n         Gradient evaluations: 39\nPoisson Regression Results (Comparison):\n                  Manual Coefficient  Statsmodels Coefficient  Std. Error  \\\nIntercept                  -0.508920                -0.508920    0.183179   \nAge                         0.148619                 0.148619    0.013869   \nAge²                       -0.002970                -0.002970    0.000258   \nregion_Northeast            0.207591                 0.207591    0.030895   \nregion_Northwest            0.029170                 0.029170    0.043625   \nregion_South               -0.017575                -0.017575    0.053781   \nregion_Southwest            0.056561                 0.056561    0.052662   \nCustomer                    0.050576                 0.050576    0.047198   \n\n                    z-value       p-value  \nIntercept         -2.778269  5.464935e-03  \nAge               10.716250  0.000000e+00  \nAge²             -11.513237  0.000000e+00  \nregion_Northeast   6.719179  1.827516e-11  \nregion_Northwest   0.668647  5.037205e-01  \nregion_South      -0.326782  7.438327e-01  \nregion_Southwest   1.074036  2.828066e-01  \nCustomer           1.071568  2.839141e-01  \n\nEffect of being a Blueprinty customer: 0.2307\nBlueprinty customers are associated with a 23.07% increase in patent count.\n\nPoisson Regression Results with MSE:\n                  Coefficient  Manual Std. Error  Statsmodels Std. Error  \\\nIntercept           -0.508920           0.181517                0.183179   \nAge                  0.148619           0.013674                0.013869   \nAge²                -0.002970           0.000252                0.000258   \nregion_Northeast     0.207591           0.030895                0.030895   \nregion_Northwest     0.029170           0.043625                0.043625   \nregion_South        -0.017575           0.053779                0.053781   \nregion_Southwest     0.056561           0.052662                0.052662   \nCustomer             0.050576           0.047198                0.047198   \n\n                    z-value       p-value  \nIntercept         -2.803698  5.052015e-03  \nAge               10.868563  0.000000e+00  \nAge²             -11.773299  0.000000e+00  \nregion_Northeast   6.719176  1.827560e-11  \nregion_Northwest   0.668647  5.037205e-01  \nregion_South      -0.326789  7.438276e-01  \nregion_Southwest   1.074045  2.828024e-01  \nCustomer           1.071572  2.839121e-01  \n\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)"
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\nvars_needed = [\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]\n\nairbnb_clean = airbnb[vars_needed].dropna()\n\n\nprint(airbnb_clean.info())\nprint(airbnb_clean.describe())\n\n# Convert price to numeric (if it's a string with $ or ,)\nairbnb_clean[\"price\"] = (\n    airbnb_clean[\"price\"]\n    .replace('[\\$,]', '', regex=True)\n    .astype(float)\n)\n\n# Convert 'instant_bookable' to binary\nairbnb_clean[\"instant_bookable\"] = airbnb_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n\n# Distribution of number of reviews\nsns.histplot(airbnb_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.show()\n\n# Boxplot of reviews by room type\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=airbnb_clean)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Fit the model\nmodel = smf.glm(\n    formula=\"\"\"\n        number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n        review_scores_cleanliness + review_scores_location +\n        review_scores_value + instant_bookable\n    \"\"\",\n    data=airbnb_clean,\n    family=sm.families.Poisson()\n).fit()\n\nprint(model.summary())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 30160 entries, 0 to 40503\nData columns (total 10 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   number_of_reviews          30160 non-null  int64  \n 1   days                       30160 non-null  int64  \n 2   room_type                  30160 non-null  object \n 3   bathrooms                  30160 non-null  float64\n 4   bedrooms                   30160 non-null  float64\n 5   price                      30160 non-null  int64  \n 6   review_scores_cleanliness  30160 non-null  float64\n 7   review_scores_location     30160 non-null  float64\n 8   review_scores_value        30160 non-null  float64\n 9   instant_bookable           30160 non-null  object \ndtypes: float64(5), int64(3), object(2)\nmemory usage: 2.5+ MB\nNone\n       number_of_reviews          days     bathrooms      bedrooms  \\\ncount       30160.000000  30160.000000  30160.000000  30160.000000   \nmean           21.170889   1139.711174      1.122132      1.151459   \nstd            32.007541   1252.303675      0.384916      0.699010   \nmin             1.000000      7.000000      0.000000      0.000000   \n25%             3.000000    584.000000      1.000000      1.000000   \n50%             8.000000   1041.000000      1.000000      1.000000   \n75%            26.000000   1592.000000      1.000000      1.000000   \nmax           421.000000  42828.000000      6.000000     10.000000   \n\n              price  review_scores_cleanliness  review_scores_location  \\\ncount  30160.000000               30160.000000            30160.000000   \nmean     140.206863                   9.201724                9.415351   \nstd      188.392314                   1.114261                0.843185   \nmin       10.000000                   2.000000                2.000000   \n25%       70.000000                   9.000000                9.000000   \n50%      103.000000                  10.000000               10.000000   \n75%      169.000000                  10.000000               10.000000   \nmax    10000.000000                  10.000000               10.000000   \n\n       review_scores_value  \ncount         30160.000000  \nmean              9.333952  \nstd               0.900472  \nmin               2.000000  \n25%               9.000000  \n50%              10.000000  \n75%              10.000000  \nmax              10.000000  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30150\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2900e+05\nDate:                Wed, 28 May 2025   Deviance:                   9.3653e+05\nTime:                        19:49:20   Pearson chi2:                 1.41e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.5649\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                     3.5725      0.016    223.215      0.000       3.541       3.604\nroom_type[T.Private room]    -0.0145      0.003     -5.310      0.000      -0.020      -0.009\nroom_type[T.Shared room]     -0.2519      0.009    -29.229      0.000      -0.269      -0.235\nbathrooms                    -0.1240      0.004    -33.091      0.000      -0.131      -0.117\nbedrooms                      0.0749      0.002     37.698      0.000       0.071       0.079\nprice                     -1.435e-05    8.3e-06     -1.729      0.084   -3.06e-05    1.92e-06\nreview_scores_cleanliness     0.1132      0.001     75.820      0.000       0.110       0.116\nreview_scores_location       -0.0768      0.002    -47.796      0.000      -0.080      -0.074\nreview_scores_value          -0.0915      0.002    -50.902      0.000      -0.095      -0.088\ninstant_bookable              0.3344      0.003    115.748      0.000       0.329       0.340\n=============================================================================================\n\n\nUsing the number of reviews as a proxy for bookings, we built a Poisson regression model to examine how listing characteristics relate to booking volume. We found that listings which are instantly bookable receive about 40% more reviews, making this the strongest positive predictor. Higher cleanliness scores also lead to significantly more reviews, with each 1-point increase associated with a 12% gain. Additional bedrooms slightly increase bookings (about 8% per bedroom), while more bathrooms surprisingly reduce them by about 12%. Shared rooms receive 22% fewer reviews than entire homes, and private rooms show no meaningful difference. Price has a negligible effect. Interestingly, higher location and value scores are associated with fewer reviews, which may reflect confounding rather than true negative effects. Overall, features that signal convenience and cleanliness appear to drive more bookings, while room type and size also play meaningful roles"
  },
  {
    "objectID": "blog/project4/index.html",
    "href": "blog/project4/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "blog/project4/index.html#a.-k-means",
    "href": "blog/project4/index.html#a.-k-means",
    "title": "Machine Learning",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# Load dataset\npenguins = pd.read_csv('palmer_penguins.csv')\ndata = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\n\n# Standardize features\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\nThe history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef initialize_centroids(X, k):\n    np.random.seed(42)\n    indices = np.random.choice(X.shape[0], size=k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans(X, k, max_iters=100, tol=1e-4, animate=False):\n    centroids = initialize_centroids(X, k)\n    for i in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) &lt; tol):\n            break\n        centroids = new_centroids\n        if animate:\n            plot_clusters(X, labels, centroids, iteration=i)\n    return labels, centroids\n\ndef plot_clusters(X, labels, centroids, iteration):\n    plt.figure()\n    for i in np.unique(labels):\n        plt.scatter(X[labels == i, 0], X[labels == i, 1], label=f'Cluster {i}')\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')\n    plt.title(f'Iteration {iteration}')\n    plt.xlabel('Bill Length (standardized)')\n    plt.ylabel('Flipper Length (standardized)')\n    plt.legend()\n    plt.show()\n\n\nlabels_custom, centroids_custom = kmeans(scaled_data, k=3, animate=True)\n\nplot_clusters(scaled_data, labels_custom, centroids_custom, iteration='Final (Custom KMeans)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHonestly, the model froms scratch looks like it did a pretty good job.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\n\nfrom sklearn.cluster import KMeans\n\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42)\nkmeans_sklearn.fit(scaled_data)\n\nplt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=kmeans_sklearn.labels_)\nplt.scatter(kmeans_sklearn.cluster_centers_[:, 0], kmeans_sklearn.cluster_centers_[:, 1], c='red', marker='x')\nplt.title('Scikit-learn KMeans Result')\nplt.xlabel('Bill Length (standardized)')\nplt.ylabel('Flipper Length (standardized)')\nplt.show()\n\n\n\n\n\n\n\n\nThe point where WCSS stops decreasing sharply indicates the best balance of compactness and simplicity\nElbow Method is showing that K=3 is the K while the Silhouette score shows K=2 to be the best with K=3 close. The higher the silhouette score the better the separation. So I would choose 3 because it’s the best for both metrics\n\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette_scores = []\nK_range = range(2, 8)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(scaled_data)\n    wcss.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(scaled_data, kmeans.labels_))\n\n# Plot WCSS\nplt.plot(K_range, wcss, marker='o')\nplt.title('Elbow Method (WCSS)')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Within-Cluster Sum of Squares')\nplt.show()\n\n# Plot Silhouette Scores\nplt.plot(K_range, silhouette_scores, marker='o')\nplt.title('Silhouette Scores')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Silhouette Score')\nplt.show()"
  },
  {
    "objectID": "blog/project4/index.html#b.-key-drivers-analysis",
    "href": "blog/project4/index.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables.\nimport pandas as pd\ndf = pd.read_csv(‘data_for_drivers_analysis.csv’)"
  }
]