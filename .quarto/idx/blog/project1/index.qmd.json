{"title":"A Replication of Karlan and List (2007)","markdown":{"yaml":{"title":"A Replication of Karlan and List (2007)","author":"Eleanor Amidei","date":"today","callout-appearance":"minimal"},"headingText":"A Replication of Karlan and List (2007)","containsRefs":false,"markdown":"\nIn their 2007 study published in the American Economic Review, economists Dean Karlan (Yale University) and John List (University of Chicago) conducted a large-scale field experiment to evaluate the impact of different types of fundraising appeals on charitable giving. The experiment was designed to test whether certain behavioral economic principles—specifically, those involving matching and challenge grants—could significantly influence donor behavior.\n\nThe researchers partnered with a nonprofit organization to send out 50,000 fundraising letters to potential donors. These individuals were randomly assigned to receive one of three types of solicitation letters:\n\nStandard Letter: This control condition contained a straightforward appeal for donations, with no mention of matching or challenge grants.\n\nMatching Grant Letter: This version of the letter informed potential donors that their contributions would be matched dollar-for-dollar by a large donor, effectively doubling the impact of each gift. The idea was to invoke a sense of increased efficacy and urgency.\n\nChallenge Grant Letter: In this version, the letter stated that a large donor had already pledged a significant amount of funding, contingent on the organization’s ability to raise additional funds from other donors. This framed the recipient’s contribution as necessary to “meet the challenge” and unlock previously pledged money.\n\nEach treatment group was randomized to ensure that differences in response could be causally attributed to the content of the letter. The researchers then tracked various outcomes, such as the likelihood of donating, the amount donated, and donor heterogeneity in response to the different appeals.\n\nThe key finding was that matching grants significantly increased both the likelihood of donating and the average donation amount, while challenge grants did not perform significantly better than the standard appeal. The results provided empirical support for the effectiveness of matching mechanisms in charitable fundraising and have since influenced both economic theory and practical strategies used by nonprofit organizations.\n\n## Data Exploration\n\n``` {python}\nimport pandas as pd\nimport numpy as np\n\nkarlan_data = pd.read_stata('karlan_list_2007.dta')\nprint(karlan_data.shape)\nprint(karlan_data.columns)\nprint(karlan_data.isnull().sum())\nprint(karlan_data.describe(include='all'))\n\nprint(karlan_data['treatment'].value_counts(normalize=True)) #treatment proportion\nprint(karlan_data['gave'].value_counts(normalize=True))  #donation rate \nprint(karlan_data['amount'].mean()) \nprint(karlan_data.dtypes)  \n\n#capping preview for notebook\npd.set_option('display.max_columns', 10)\n\n\n```\n\n\n## Balance Test \nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nWhen doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._\n\nI will be testing the following variables to ensure that the treatment and control groups are statistically similar:\n- `mrm2`: The number of months since the last donation.\n- `bluecty` : If the potential donor lives in a blue county.\n- `freq`: The number of prior donations.\n- `female`: The gender to the donor.\n\n#### `mrm2`\n``` {python}\n\nfrom scipy import stats\n\n# mrm2\n# groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['mrm2'].dropna()\n\n\n# Manual calculation of t-statistic\n\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Degrees of freedom (Welch's approximation)\ndf = (var1/n1 + var2/n2)**2 / ((var1**2)/((n1**2)*(n1 - 1)) + (var2**2)/((n2**2)*(n2 - 1)))\n\n# Two-tailed p-value\nfrom scipy.stats import t\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\n\n\nprint(\"Manual p-value:\", p_value)\n\n\n```\nWith a t-statistic of .12, which is not more extreme than 1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level. \n\nAdditionally, when we run this t-statistic through a pre-built program, we find the p_value is actually .905, which much higher than the .05 threshold we would need to reject the null hypothesis.\n\n``` {python}\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"mrm2\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n```\n\nThe positive t-statistic is minimal but indicates the treatment group has a slightly higher number of months since last donation. The p-value is well above 0.05, indicating that this difference is not statistically significant at the 95% confidence level. These are the same results we saw with our manual t-test calculation. \n\n#### `bluectyc`\n\n``` {python}\n#bluecty - donor county is blue\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['bluecty'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['bluecty'].dropna()\n\n#t-test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n```\n\nThe t-statistic is -0.85, which is not more extreme than -1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level for the blue county variable.\n\n``` {python}\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"bluecty\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n```\n\nWe see the same t-statistic here, and the p-value is .396, which is well above the .05 threshold we would need to reject the null hypothesis.\n\n#### `freq`\n\n``` {python}\n#freq - the number of prior donations\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['freq'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['freq'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"freq\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\n```\n\nThere is no significant difference between the treatment and control groups in terms of the number of prior donations. The t-statistic is very close to zero, and the p-value is well above 0.05, indicating no statistically significant difference at the 95% confidence level. This is consistent across both the t-test and linear regression.\n\n#### `female`\n``` {python}\n\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['female'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['female'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"female\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n```\n\nHere the t-statistic generated from the manual test is close to -1.96, but still less extreme at -1.75. This means we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level.\n\nWhen we run this through a pre-built program, we find the p-value is .08, which is above the .05 threshold but below the .10 threshold.\n\nThe difference between the percentage of female potential donors is marginally different between the two groups, this difference is not significant at the 95% confidence level. This is true in both the t-test and the linear regression. This variable however does have a significant difference at the 90% confidence level, so it would be worth noting. \n\n## Charitable Contribution Made\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\n``` {python}\nimport matplotlib.pyplot as plt\n\ndonated_treatment = karlan_data[karlan_data['treatment'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_treatment*100,2)}% of treatment group donated\")\ndonated_control = karlan_data[karlan_data['control'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_control*100,2)}% of control group donated\")\n\nplt.figure(figsize=(8, 6))\nplt.bar(['Treatment', 'Control'], [donated_treatment, donated_control], color=['green', 'pink'])\nplt.title('Donation Rates by Group')\nplt.ylabel('Donation Rate')\n\n```\n\n``` {python}\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['gave']\ncontrol_group = karlan_data[karlan_data['control'] == 1]['gave']\n\n# t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"gave\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\n#probit regression\nimport statsmodels.api as sm\n\nX = karlan_data['treatment']\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\n```\n\nThere difference between the treatment and control group's response rate is significant at the 95% confidence level (the t-value is more extreme than 1.96 for a two-tailed test). The treatment group has a higher response rate than the control group, which suggests that the matching grant appeal is effective in increasing the likelihood of making a donation. This finding aligns with the hypothesis that matching grants can enhance donor motivation and engagement.\n\n## Differences between Match Rates\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n1:1 vs 2:1 match ratio on response rate\n\n``` {python}\n\n# t-test comparing 1:1 match rate and 2:1 match rate\n\n#groups\nmatch_1_1 = karlan_data[karlan_data['ratio'] == 1]['gave']\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_1_1), len(match_2_1)\nmean1, mean2 = np.mean(match_1_1), np.mean(match_2_1)\nvar1, var2 = np.var(match_1_1, ddof=1), np.var(match_2_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\n\nprint(\"Manual t-statistic:\", t_manual)\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n```\n\n2:1 vs 3:1 match ratio on response rate\n\n``` {python}\n# t-test comparing 2:1 match rate and 3:1 match rate\n#groups\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\nmatch_3_1 = karlan_data[karlan_data['ratio3'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_2_1), len(match_3_1)\nmean1, mean2 = np.mean(match_2_1), np.mean(match_3_1)\nvar1, var2 = np.var(match_2_1, ddof=1), np.var(match_3_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n```\n\nThe negative t-statistic indicates that the 2:1 match ratio has a higher mean that the 1:1 ratio, however this difference is not significant at the 95% confidence level. The p-value is .34, which is well above the .05 threshold we would need to reject the null hypothesis.\n\nThe same applies to the 3:1 match ratio, with a p-value of .96 and a smaller difference between the two groups. \n\n_todo: Assess the same issue using a regression. Specifically, create the variable `ratio1` then regress `gave` on `ratio1`, `ratio2`, and `ratio3` (or alternatively, regress `gave` on the categorical variable `ratio`). Interpret the coefficients and their statistical precision._\n\n``` {python}\nkarlan_data[\"ratio1\"] = (karlan_data[\"ratio\"] == 1).astype(int)\n\nX = karlan_data[['ratio1','ratio2','ratio3']]\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\n\n```\n\n1:1 ratio shows a coefficient 0.062, meaning the 1:1 ratio is less likely to lead to a donation than the other ratio's but a match should still increase the probability for a give. The 2:1 ratio has an odds ratio of 0.0980, meaning it is slightly more likely to lead to a donation than the control group. The 3:1 ratio has an odds ratio of 0.0998, meaning it is slightly more likely to lead to a donation than the 2:1 ratio. The p_values indicate that the results for ratio 2 and ratio 3 are statistically significant at the 95% confidence level, while the results for ratio 1 are not.\n\nCalculating the response rate differences between match ratios: \n\n- calculated directly from the data\n\n``` {python}\n#means\nmean_1_1 = karlan_data[karlan_data[\"ratio\"] == 1][\"gave\"].mean()\nmean_2_1 = karlan_data[karlan_data[\"ratio\"] == 2][\"gave\"].mean()\nmean_3_1 = karlan_data[karlan_data[\"ratio\"] == 3][\"gave\"].mean()\n\n#differences\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\nprint(\"2:1 vs 1:1:\", diff_2_1_vs_1_1)\nprint(\"3:1 vs 2:1:\", diff_3_1_vs_2_1)\n\n```\n\n - calculated from the differences in coefficients\n\n ``` {python}\ncoef_1_1 = result.params['ratio1']\ncoef_2_1 = result.params['ratio2']\ncoef_3_1 = result.params['ratio3']\n\n# Difference in effects\ndiff_2_1_vs_1_1 = coef_2_1 - coef_1_1\ndiff_3_1_vs_2_1 = coef_3_1 - coef_2_1\nprint(\"Regression-based effect of 3:1 vs 2:1 match:\", diff_3_1_vs_2_1)\nprint(\"Regression-based effect of 2:1 vs 1:1 match:\", diff_2_1_vs_1_1)\n\nmfx = result.get_margeff()\nprint(mfx.summary())\n\n ```\n\nBoth raw and model-based results suggest that increasing the match ratio from 1:1 to 2:1 significantly improves donation rates. However, increasing it further to 3:1 provides almost no additional benefit. These findings support the paper’s interpretation that higher match ratios can increase giving, but they also highlight diminishing returns at higher match levels.\n\nThe p-values for ratio1 are significant at the 90% level but the results for ratio2 and ratio3 are significant at the 95% level..\n\nBased on the probit model, the 3:1 match leads to a slightly higher (0.0018) latent index score for donation compared to the 2:1 match, holding everything else constant.\n\n\n## Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution (amount of donation).\n\n``` {python}\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n```\n\nIn the regression above, we learn that treatment effect has a slightly significant (not at the 95% confidence level and small) effect on size of donation.\n\n``` {python}\nkarlan_donations = karlan_data[karlan_data['gave'] ==1 ]\nreg = rsm.model.regress({\"karlan\": karlan_donations}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n```\n\nWhen only including donors, the coefficient of the regression shows us the treatment group donates about $1.67 less than the control group, however this result is not significant at the 95% confidence level. This does not have a causal interpretation because treatment may affect the likelihood of donating, and here we're looking at the size of the donation conditional on donating.\n\n``` {python}\ndonors = karlan_data[karlan_data['gave'] ==1]\n\n# Control\ncontrol_donors = donors[donors['treatment'] == 0]['amount']\nplt.hist(control_donors, bins=30, alpha=0.7, label='Control')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Control Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Treatment\ntreated_donors = donors[donors['treatment'] == 1]['amount']\nplt.hist(treated_donors, bins=30, alpha=0.7, label='Treatment')\nplt.axvline(treated_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Treatment Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n```\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\nHere, I simulate 10,000 draws from the control and treatment groups, then calculate their differences.\n\n\n``` {python}\n#Simulated Numbers\nnp.random.seed(1)  # For reproducibilit\nsim_control = np.random.choice(control_donors, size=10000, replace=True)\nsim_treated = np.random.choice(treated_donors, size=10000, replace=True)\ndiff=sim_treated-sim_control\n\n#Cumulative Average\ncumulative_avg = np.cumsum(diff) / np.arange(1, len(diff) + 1)\n\n#True Difference in means\nTrue_Diff = treated_donors.mean() - control_donors.mean()\n\nplt.plot(cumulative_avg, label='Cumulative Average')\nplt.axhline(True_Diff, color='red', linestyle='dashed', linewidth=2, label='True Difference')\nplt.title('Cumulative Average of Differences')\nplt.xlabel('Number of Samples')\nplt.ylabel('Cumulative Average')\nplt.legend()\n\n```\n\nBy plotting the cumulative average, we can see it approaches the true difference in means. As we increase the number of samples, the variation stabilizes. The cumulative average converges to the true difference in means. This demonstrates the Law of Large Numbers, as the sample mean approaches the population mean as the sample size increases.\n\n### Central Limit Theorem\nBelow are 4 histograms with the difference between the control and treatment group in samples sizes of 50,100,150,and 200. We then repeat the sampling 1000x to see the averages\n\n\n``` {python}\np_control = 0.018\np_treatment = 0.022\n\n# parameters\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\ntrue_diff = p_treatment - p_control\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\n# Simulate and plot \nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_diffs.append(avg_diff)\n\n    axs[i].hist(avg_diffs, bins=30, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n    axs[i].axvline(x=true_diff, color='red', linestyle='--', label='True Difference (0.004)')\n    axs[i].axvline(x=0, color='black', linestyle=':', label='Zero')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg Treatment - Control\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\n# layout\nplt.suptitle(\"CLT: Distribution of Average Differences by Sample Size\", fontsize=14)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n````\n\nThe four histograms above illustrate how the distribution of average differences in donation rates between treatment and control groups evolves as sample size increases. This simulation confirms the Central Limit Theorem (CLT).\n\nAt n = 50, the distribution is wide and relatively flat, and zero lies close to the center. This shows that with small sample sizes, we often can't distinguish signal from noise, and random variation makes the estimated treatment effect unreliable.\n\nAt n = 200, the distribution becomes more symmetric and bell-shaped. The true treatment effect (0.004) begins to emerge, though zero is still within the central bulk of the distribution, indicating moderate uncertainty.\n\nAt n = 500, the distribution tightens further, and the center of the histogram clearly shifts to the right of zero. Zero now lies toward the edge (tail) of the distribution, which suggests that the true effect is increasingly distinguishable from no effect.\n\nAt n = 1000, the distribution is even narrower and sharply centered near 0.004. Zero is clearly in the tail, meaning that under this sample size, the true effect is more evident.\n\nConclusion:\nAs sample size increases, the sampling distribution of the average difference becomes more normal and less variable, with its mean converging to the true treatment effect. Zero moves from the center to the tails of the distribution, reinforcing that larger samples improve the precision of effect estimates and the reliability of hypothesis testing.","srcMarkdownNoYaml":"\n# A Replication of Karlan and List (2007)\nIn their 2007 study published in the American Economic Review, economists Dean Karlan (Yale University) and John List (University of Chicago) conducted a large-scale field experiment to evaluate the impact of different types of fundraising appeals on charitable giving. The experiment was designed to test whether certain behavioral economic principles—specifically, those involving matching and challenge grants—could significantly influence donor behavior.\n\nThe researchers partnered with a nonprofit organization to send out 50,000 fundraising letters to potential donors. These individuals were randomly assigned to receive one of three types of solicitation letters:\n\nStandard Letter: This control condition contained a straightforward appeal for donations, with no mention of matching or challenge grants.\n\nMatching Grant Letter: This version of the letter informed potential donors that their contributions would be matched dollar-for-dollar by a large donor, effectively doubling the impact of each gift. The idea was to invoke a sense of increased efficacy and urgency.\n\nChallenge Grant Letter: In this version, the letter stated that a large donor had already pledged a significant amount of funding, contingent on the organization’s ability to raise additional funds from other donors. This framed the recipient’s contribution as necessary to “meet the challenge” and unlock previously pledged money.\n\nEach treatment group was randomized to ensure that differences in response could be causally attributed to the content of the letter. The researchers then tracked various outcomes, such as the likelihood of donating, the amount donated, and donor heterogeneity in response to the different appeals.\n\nThe key finding was that matching grants significantly increased both the likelihood of donating and the average donation amount, while challenge grants did not perform significantly better than the standard appeal. The results provided empirical support for the effectiveness of matching mechanisms in charitable fundraising and have since influenced both economic theory and practical strategies used by nonprofit organizations.\n\n## Data Exploration\n\n``` {python}\nimport pandas as pd\nimport numpy as np\n\nkarlan_data = pd.read_stata('karlan_list_2007.dta')\nprint(karlan_data.shape)\nprint(karlan_data.columns)\nprint(karlan_data.isnull().sum())\nprint(karlan_data.describe(include='all'))\n\nprint(karlan_data['treatment'].value_counts(normalize=True)) #treatment proportion\nprint(karlan_data['gave'].value_counts(normalize=True))  #donation rate \nprint(karlan_data['amount'].mean()) \nprint(karlan_data.dtypes)  \n\n#capping preview for notebook\npd.set_option('display.max_columns', 10)\n\n\n```\n\n\n## Balance Test \nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nWhen doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._\n\nI will be testing the following variables to ensure that the treatment and control groups are statistically similar:\n- `mrm2`: The number of months since the last donation.\n- `bluecty` : If the potential donor lives in a blue county.\n- `freq`: The number of prior donations.\n- `female`: The gender to the donor.\n\n#### `mrm2`\n``` {python}\n\nfrom scipy import stats\n\n# mrm2\n# groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['mrm2'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['mrm2'].dropna()\n\n\n# Manual calculation of t-statistic\n\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Degrees of freedom (Welch's approximation)\ndf = (var1/n1 + var2/n2)**2 / ((var1**2)/((n1**2)*(n1 - 1)) + (var2**2)/((n2**2)*(n2 - 1)))\n\n# Two-tailed p-value\nfrom scipy.stats import t\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\n\n\nprint(\"Manual p-value:\", p_value)\n\n\n```\nWith a t-statistic of .12, which is not more extreme than 1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level. \n\nAdditionally, when we run this t-statistic through a pre-built program, we find the p_value is actually .905, which much higher than the .05 threshold we would need to reject the null hypothesis.\n\n``` {python}\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"mrm2\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n```\n\nThe positive t-statistic is minimal but indicates the treatment group has a slightly higher number of months since last donation. The p-value is well above 0.05, indicating that this difference is not statistically significant at the 95% confidence level. These are the same results we saw with our manual t-test calculation. \n\n#### `bluectyc`\n\n``` {python}\n#bluecty - donor county is blue\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['bluecty'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['bluecty'].dropna()\n\n#t-test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n```\n\nThe t-statistic is -0.85, which is not more extreme than -1.96, we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level for the blue county variable.\n\n``` {python}\nimport pyrsm as rsm \n\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"bluecty\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n```\n\nWe see the same t-statistic here, and the p-value is .396, which is well above the .05 threshold we would need to reject the null hypothesis.\n\n#### `freq`\n\n``` {python}\n#freq - the number of prior donations\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['freq'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['freq'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"freq\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\n```\n\nThere is no significant difference between the treatment and control groups in terms of the number of prior donations. The t-statistic is very close to zero, and the p-value is well above 0.05, indicating no statistically significant difference at the 95% confidence level. This is consistent across both the t-test and linear regression.\n\n#### `female`\n``` {python}\n\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['female'].dropna()\ncontrol_group = karlan_data[karlan_data['control'] == 1]['female'].dropna()\n\n# manual t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"female\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n```\n\nHere the t-statistic generated from the manual test is close to -1.96, but still less extreme at -1.75. This means we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different from one another at the 95% confidence level.\n\nWhen we run this through a pre-built program, we find the p-value is .08, which is above the .05 threshold but below the .10 threshold.\n\nThe difference between the percentage of female potential donors is marginally different between the two groups, this difference is not significant at the 95% confidence level. This is true in both the t-test and the linear regression. This variable however does have a significant difference at the 90% confidence level, so it would be worth noting. \n\n## Charitable Contribution Made\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\n``` {python}\nimport matplotlib.pyplot as plt\n\ndonated_treatment = karlan_data[karlan_data['treatment'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_treatment*100,2)}% of treatment group donated\")\ndonated_control = karlan_data[karlan_data['control'] == 1][\"gave\"].mean()\nprint(f\"{round(donated_control*100,2)}% of control group donated\")\n\nplt.figure(figsize=(8, 6))\nplt.bar(['Treatment', 'Control'], [donated_treatment, donated_control], color=['green', 'pink'])\nplt.title('Donation Rates by Group')\nplt.ylabel('Donation Rate')\n\n```\n\n``` {python}\n#groups\ntreatment_group = karlan_data[karlan_data['treatment'] == 1]['gave']\ncontrol_group = karlan_data[karlan_data['control'] == 1]['gave']\n\n# t_test\nn1, n2 = len(treatment_group), len(control_group)\nmean1, mean2 = np.mean(treatment_group), np.mean(control_group)\nvar1, var2 = np.var(treatment_group, ddof=1), np.var(control_group, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n\n\n#linear regression\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"gave\", evar=[\"control\"])\nreg.summary(rmse=True, ssq=True)\n\n#probit regression\nimport statsmodels.api as sm\n\nX = karlan_data['treatment']\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\n```\n\nThere difference between the treatment and control group's response rate is significant at the 95% confidence level (the t-value is more extreme than 1.96 for a two-tailed test). The treatment group has a higher response rate than the control group, which suggests that the matching grant appeal is effective in increasing the likelihood of making a donation. This finding aligns with the hypothesis that matching grants can enhance donor motivation and engagement.\n\n## Differences between Match Rates\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n1:1 vs 2:1 match ratio on response rate\n\n``` {python}\n\n# t-test comparing 1:1 match rate and 2:1 match rate\n\n#groups\nmatch_1_1 = karlan_data[karlan_data['ratio'] == 1]['gave']\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_1_1), len(match_2_1)\nmean1, mean2 = np.mean(match_1_1), np.mean(match_2_1)\nvar1, var2 = np.var(match_1_1, ddof=1), np.var(match_2_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\n\nprint(\"Manual t-statistic:\", t_manual)\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n```\n\n2:1 vs 3:1 match ratio on response rate\n\n``` {python}\n# t-test comparing 2:1 match rate and 3:1 match rate\n#groups\nmatch_2_1 = karlan_data[karlan_data['ratio2'] == 1]['gave']\nmatch_3_1 = karlan_data[karlan_data['ratio3'] == 1]['gave']\n\n# t_test\nn1, n2 = len(match_2_1), len(match_3_1)\nmean1, mean2 = np.mean(match_2_1), np.mean(match_3_1)\nvar1, var2 = np.var(match_2_1, ddof=1), np.var(match_3_1, ddof=1)\n\n# Standard error\nse = np.sqrt(var1/n1 + var2/n2)\n\n# t-stat\nt_manual = (mean1 - mean2) / se\nprint(\"Manual t-statistic:\", t_manual)\n\n# Two-tailed p-value\n\np_value = 2 * (1 - t.cdf(abs(t_manual), df))\nprint(\"Manual p-value:\", p_value)\n```\n\nThe negative t-statistic indicates that the 2:1 match ratio has a higher mean that the 1:1 ratio, however this difference is not significant at the 95% confidence level. The p-value is .34, which is well above the .05 threshold we would need to reject the null hypothesis.\n\nThe same applies to the 3:1 match ratio, with a p-value of .96 and a smaller difference between the two groups. \n\n_todo: Assess the same issue using a regression. Specifically, create the variable `ratio1` then regress `gave` on `ratio1`, `ratio2`, and `ratio3` (or alternatively, regress `gave` on the categorical variable `ratio`). Interpret the coefficients and their statistical precision._\n\n``` {python}\nkarlan_data[\"ratio1\"] = (karlan_data[\"ratio\"] == 1).astype(int)\n\nX = karlan_data[['ratio1','ratio2','ratio3']]\nY = karlan_data['gave']\n\nX = sm.add_constant(X)\n\n# probit model\nprobit_model = sm.Probit(Y, X)\nresult = probit_model.fit()\n\nprint(result.summary())\n\n#marginal effects to see if results match from the study\nmfx = result.get_margeff()\nprint(mfx.summary())\n\n\n```\n\n1:1 ratio shows a coefficient 0.062, meaning the 1:1 ratio is less likely to lead to a donation than the other ratio's but a match should still increase the probability for a give. The 2:1 ratio has an odds ratio of 0.0980, meaning it is slightly more likely to lead to a donation than the control group. The 3:1 ratio has an odds ratio of 0.0998, meaning it is slightly more likely to lead to a donation than the 2:1 ratio. The p_values indicate that the results for ratio 2 and ratio 3 are statistically significant at the 95% confidence level, while the results for ratio 1 are not.\n\nCalculating the response rate differences between match ratios: \n\n- calculated directly from the data\n\n``` {python}\n#means\nmean_1_1 = karlan_data[karlan_data[\"ratio\"] == 1][\"gave\"].mean()\nmean_2_1 = karlan_data[karlan_data[\"ratio\"] == 2][\"gave\"].mean()\nmean_3_1 = karlan_data[karlan_data[\"ratio\"] == 3][\"gave\"].mean()\n\n#differences\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\nprint(\"2:1 vs 1:1:\", diff_2_1_vs_1_1)\nprint(\"3:1 vs 2:1:\", diff_3_1_vs_2_1)\n\n```\n\n - calculated from the differences in coefficients\n\n ``` {python}\ncoef_1_1 = result.params['ratio1']\ncoef_2_1 = result.params['ratio2']\ncoef_3_1 = result.params['ratio3']\n\n# Difference in effects\ndiff_2_1_vs_1_1 = coef_2_1 - coef_1_1\ndiff_3_1_vs_2_1 = coef_3_1 - coef_2_1\nprint(\"Regression-based effect of 3:1 vs 2:1 match:\", diff_3_1_vs_2_1)\nprint(\"Regression-based effect of 2:1 vs 1:1 match:\", diff_2_1_vs_1_1)\n\nmfx = result.get_margeff()\nprint(mfx.summary())\n\n ```\n\nBoth raw and model-based results suggest that increasing the match ratio from 1:1 to 2:1 significantly improves donation rates. However, increasing it further to 3:1 provides almost no additional benefit. These findings support the paper’s interpretation that higher match ratios can increase giving, but they also highlight diminishing returns at higher match levels.\n\nThe p-values for ratio1 are significant at the 90% level but the results for ratio2 and ratio3 are significant at the 95% level..\n\nBased on the probit model, the 3:1 match leads to a slightly higher (0.0018) latent index score for donation compared to the 2:1 match, holding everything else constant.\n\n\n## Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution (amount of donation).\n\n``` {python}\nreg = rsm.model.regress({\"karlan\": karlan_data}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n```\n\nIn the regression above, we learn that treatment effect has a slightly significant (not at the 95% confidence level and small) effect on size of donation.\n\n``` {python}\nkarlan_donations = karlan_data[karlan_data['gave'] ==1 ]\nreg = rsm.model.regress({\"karlan\": karlan_donations}, rvar=\"amount\", evar=[\"treatment\"])\nreg.summary()\n```\n\nWhen only including donors, the coefficient of the regression shows us the treatment group donates about $1.67 less than the control group, however this result is not significant at the 95% confidence level. This does not have a causal interpretation because treatment may affect the likelihood of donating, and here we're looking at the size of the donation conditional on donating.\n\n``` {python}\ndonors = karlan_data[karlan_data['gave'] ==1]\n\n# Control\ncontrol_donors = donors[donors['treatment'] == 0]['amount']\nplt.hist(control_donors, bins=30, alpha=0.7, label='Control')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Control Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Treatment\ntreated_donors = donors[donors['treatment'] == 1]['amount']\nplt.hist(treated_donors, bins=30, alpha=0.7, label='Treatment')\nplt.axvline(treated_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title(\"Treatment Group: Donation Amounts\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n```\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\nHere, I simulate 10,000 draws from the control and treatment groups, then calculate their differences.\n\n\n``` {python}\n#Simulated Numbers\nnp.random.seed(1)  # For reproducibilit\nsim_control = np.random.choice(control_donors, size=10000, replace=True)\nsim_treated = np.random.choice(treated_donors, size=10000, replace=True)\ndiff=sim_treated-sim_control\n\n#Cumulative Average\ncumulative_avg = np.cumsum(diff) / np.arange(1, len(diff) + 1)\n\n#True Difference in means\nTrue_Diff = treated_donors.mean() - control_donors.mean()\n\nplt.plot(cumulative_avg, label='Cumulative Average')\nplt.axhline(True_Diff, color='red', linestyle='dashed', linewidth=2, label='True Difference')\nplt.title('Cumulative Average of Differences')\nplt.xlabel('Number of Samples')\nplt.ylabel('Cumulative Average')\nplt.legend()\n\n```\n\nBy plotting the cumulative average, we can see it approaches the true difference in means. As we increase the number of samples, the variation stabilizes. The cumulative average converges to the true difference in means. This demonstrates the Law of Large Numbers, as the sample mean approaches the population mean as the sample size increases.\n\n### Central Limit Theorem\nBelow are 4 histograms with the difference between the control and treatment group in samples sizes of 50,100,150,and 200. We then repeat the sampling 1000x to see the averages\n\n\n``` {python}\np_control = 0.018\np_treatment = 0.022\n\n# parameters\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\ntrue_diff = p_treatment - p_control\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\n# Simulate and plot \nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_diffs.append(avg_diff)\n\n    axs[i].hist(avg_diffs, bins=30, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n    axs[i].axvline(x=true_diff, color='red', linestyle='--', label='True Difference (0.004)')\n    axs[i].axvline(x=0, color='black', linestyle=':', label='Zero')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg Treatment - Control\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\n# layout\nplt.suptitle(\"CLT: Distribution of Average Differences by Sample Size\", fontsize=14)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n````\n\nThe four histograms above illustrate how the distribution of average differences in donation rates between treatment and control groups evolves as sample size increases. This simulation confirms the Central Limit Theorem (CLT).\n\nAt n = 50, the distribution is wide and relatively flat, and zero lies close to the center. This shows that with small sample sizes, we often can't distinguish signal from noise, and random variation makes the estimated treatment effect unreliable.\n\nAt n = 200, the distribution becomes more symmetric and bell-shaped. The true treatment effect (0.004) begins to emerge, though zero is still within the central bulk of the distribution, indicating moderate uncertainty.\n\nAt n = 500, the distribution tightens further, and the center of the histogram clearly shifts to the right of zero. Zero now lies toward the edge (tail) of the distribution, which suggests that the true effect is increasingly distinguishable from no effect.\n\nAt n = 1000, the distribution is even narrower and sharply centered near 0.004. Zero is clearly in the tail, meaning that under this sample size, the true effect is more evident.\n\nConclusion:\nAs sample size increases, the sampling distribution of the average difference becomes more normal and less variable, with its mean converging to the true treatment effect. Zero moves from the center to the tails of the distribution, reinforcing that larger samples improve the precision of effect estimates and the reliability of hypothesis testing."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","theme":["cosmo","brand"],"title":"A Replication of Karlan and List (2007)","author":"Eleanor Amidei","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}