{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Eleanor Amidei","date":"today","callout-appearance":"minimal"},"headingText":"Blueprinty Case Study","containsRefs":false,"markdown":"\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n```{python}\nimport pandas as pd\nimport numpy as np\n\nairbnb = pd.read_csv('airbnb.csv')\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', kde=False, bins=30, multiple='dodge')\nplt.title('Distribution of Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\nblueprinty.groupby('iscustomer')['patents'].mean()\n\nprint(blueprinty.groupby('iscustomer')['patents'].mean())\n\n# T-test \nfrom scipy.stats import ttest_ind\n\ncust = blueprinty[blueprinty['iscustomer'] == 1]['patents']\nnoncust = blueprinty[blueprinty['iscustomer'] == 0]['patents']\n\nt_stat, p_val = ttest_ind(cust, noncust, equal_var=False)\nprint(f\"T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}\")\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('patents ~ iscustomer + age + C(region)', data=blueprinty).fit()\nprint(model.summary())\n\n```\nTest of means:\n\nBlueprinty customers have significantly more patents than non-customers on average (4.13 vs. 3.47), and the difference is statistically significant (p < 0.001). However, this is an unadjusted comparison and may be influenced by other factors like age or region.\n\n\n1. Customer status is assiociated with more patients: \n\niscustomer coefficient = +0.641, p < 0.001\n\nInterpretation: Controlling for age and region, Blueprinty customers have, on average, 0.64 more patents than non-customers.\n\nStatistically significant at the 0.1% level → this relationship is unlikely to be due to chance.\n\n2. Age Has a Negative Effect:\n\nage coefficient = -0.036, p < 0.001\n\nInterpretation: For each additional year of company age, patent count decreases slightly (by 0.036).\n\nThis might suggest younger companies are more innovative, or older ones already hold established portfolios.\n\n3. Region Doesn’t Seem to Matter Much\nAll region coefficients are statistically insignificant (p > 0.05).\n\nNo strong evidence that region has a meaningful effect on patent count in this sample.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n_todo: Write down mathematically the likelihood for_ $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n\n_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n\n```\npoisson_loglikelihood <- function(lambda, Y){\n   ...\n}\n```\n\n_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._\n\n_todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which \"feels right\" because the mean of a Poisson distribution is lambda._\n\n_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln  \nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda <= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\n# Use actual Y values from the blueprinty dataset\nY = blueprinty['patents'].values\n\n# Lambda range for plotting\nlambdas = np.linspace(0.1, 20, 200)\nlogliks = [poisson_loglikelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambdas, logliks)\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n# We minimize the negative log-likelihood\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = optimize.minimize_scalar(neg_loglik, bounds=(0.01, 50), method='bounded')\nlambda_mle = result.x\n\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y: {np.mean(Y):.4f}\")\n\n```\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\\lambda_i = e^{X_i'\\beta}$. _For example:_\n\n```\npoisson_regression_likelihood <- function(beta, Y, X){\n   ...\n}\n```\n\n_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n_todo: Check your results using R's glm() function or Python sm.GLM() function._\n\n_todo: Interpret the results._ \n\n_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._\n\n```{python}\nimport numpy as np\nimport pandas as pd\nimport scipy.special\nimport scipy.optimize\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Poisson\nfrom scipy import stats\n\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.array(beta)\n    Y = np.array(Y)\n    X = np.array(X)\n    \n    linear_pred = X.dot(beta)\n    \n    linear_pred = np.clip(linear_pred, -30, 30)\n    \n    lambda_i = np.exp(linear_pred)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i + 1e-10) - lambda_i - scipy.special.gammaln(Y + 1))\n    \n    return log_likelihood\n\ndef negative_poisson_loglikelihood(beta, Y, X):\n    return -poisson_loglikelihood(beta, Y, X)\n\nblueprinty['age_squared'] = blueprinty['age'].astype(float) ** 2\nregion_dummies = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)\n\nX_data = pd.DataFrame()\nX_data['age'] = blueprinty['age'].astype(float)\nX_data['age_squared'] = blueprinty['age_squared'].astype(float)\nX_data['iscustomer'] = blueprinty['iscustomer'].astype(float)\n\nfor col in region_dummies.columns:\n    X_data[col] = region_dummies[col].astype(float)\n\nX = sm.add_constant(X_data)\nY = blueprinty['patents'].astype(float)\n\nX_array = np.asarray(X)\nY_array = np.asarray(Y)\n\npoisson_model = sm.GLM(Y_array, X_array, family=Poisson())\npoisson_results = poisson_model.fit()\ninitial_beta = poisson_results.params\n\nresult = scipy.optimize.minimize(\n    negative_poisson_loglikelihood,\n    initial_beta,\n    args=(Y_array, X_array),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_mle = result.x\n\nstd_errors = poisson_results.bse\n\ncolumn_names = ['Intercept', 'Age', 'Age²']\nregion_cols = list(region_dummies.columns)\ncolumn_names.extend(region_cols)\ncolumn_names.append('Customer')\n\ncomparison_df = pd.DataFrame({\n    'Manual Coefficient': beta_mle,\n    'Statsmodels Coefficient': poisson_results.params,\n    'Std. Error': std_errors,\n    'z-value': poisson_results.params / std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(poisson_results.params / std_errors)))\n})\ncomparison_df.index = column_names[:len(beta_mle)]\n\nprint(\"Poisson Regression Results (Comparison):\")\nprint(comparison_df)\n\niscustomer_idx = list(X.columns).index('iscustomer')\ncustomer_effect = np.exp(poisson_results.params[iscustomer_idx]) - 1\nprint(f\"\\nEffect of being a Blueprinty customer: {customer_effect:.4f}\")\nprint(f\"Blueprinty customers are associated with a {customer_effect*100:.2f}% increase in patent count.\")\n\nfrom scipy.optimize import approx_fprime\n\ndef hessian(func, x, *args):\n    n = len(x)\n    h = 1e-5 \n    hessy = np.zeros((n, n))\n    \n    def grad(x, *args):\n        return approx_fprime(x, func, h, *args)\n    \n    for i in range(n):\n        x_plus = x.copy()\n        x_plus[i] += h\n        grad_plus = grad(x_plus, *args)\n        \n        grad_x = grad(x, *args)\n        \n        hessy[i] = (grad_plus - grad_x) / h\n    \n    hessy = (hessy + hessy.T) / 2\n    \n    return hessy\n\nhessy = hessian(negative_poisson_loglikelihood, beta_mle, Y_array, X_array)\n\ncov_matrix = np.linalg.inv(hessy)\nmanual_std_errors = np.sqrt(np.diag(cov_matrix))\n\nmanual_results_df = pd.DataFrame({\n    'Coefficient': beta_mle,\n    'Manual Std. Error': manual_std_errors,\n    'Statsmodels Std. Error': std_errors,\n    'z-value': beta_mle / manual_std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(beta_mle / manual_std_errors)))\n})\nmanual_results_df.index = column_names[:len(beta_mle)]\n\nprint(\"\\nPoisson Regression Results with MSE:\")\nprint(manual_results_df)\n```\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n``` {python}\n\n\n\nvars_needed = [\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]\n\nairbnb_clean = airbnb[vars_needed].dropna()\n\n\nprint(airbnb_clean.info())\nprint(airbnb_clean.describe())\n\n# Convert price to numeric (if it's a string with $ or ,)\nairbnb_clean[\"price\"] = (\n    airbnb_clean[\"price\"]\n    .replace('[\\$,]', '', regex=True)\n    .astype(float)\n)\n\n# Convert 'instant_bookable' to binary\nairbnb_clean[\"instant_bookable\"] = airbnb_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n\n# Distribution of number of reviews\nsns.histplot(airbnb_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.show()\n\n# Boxplot of reviews by room type\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=airbnb_clean)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Fit the model\nmodel = smf.glm(\n    formula=\"\"\"\n        number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n        review_scores_cleanliness + review_scores_location +\n        review_scores_value + instant_bookable\n    \"\"\",\n    data=airbnb_clean,\n    family=sm.families.Poisson()\n).fit()\n\nprint(model.summary())\n\n```\n\nUsing the number of reviews as a proxy for bookings, we built a Poisson regression model to examine how listing characteristics relate to booking volume. We found that listings which are instantly bookable receive about 40% more reviews, making this the strongest positive predictor. Higher cleanliness scores also lead to significantly more reviews, with each 1-point increase associated with a 12% gain. Additional bedrooms slightly increase bookings (about 8% per bedroom), while more bathrooms surprisingly reduce them by about 12%. Shared rooms receive 22% fewer reviews than entire homes, and private rooms show no meaningful difference. Price has a negligible effect. Interestingly, higher location and value scores are associated with fewer reviews, which may reflect confounding rather than true negative effects. Overall, features that signal convenience and cleanliness appear to drive more bookings, while room type and size also play meaningful roles","srcMarkdownNoYaml":"\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n```{python}\nimport pandas as pd\nimport numpy as np\n\nairbnb = pd.read_csv('airbnb.csv')\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head()\n\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nsns.histplot(data=blueprinty, x='patents', hue='iscustomer', kde=False, bins=30, multiple='dodge')\nplt.title('Distribution of Number of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\nblueprinty.groupby('iscustomer')['patents'].mean()\n\nprint(blueprinty.groupby('iscustomer')['patents'].mean())\n\n# T-test \nfrom scipy.stats import ttest_ind\n\ncust = blueprinty[blueprinty['iscustomer'] == 1]['patents']\nnoncust = blueprinty[blueprinty['iscustomer'] == 0]['patents']\n\nt_stat, p_val = ttest_ind(cust, noncust, equal_var=False)\nprint(f\"T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}\")\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('patents ~ iscustomer + age + C(region)', data=blueprinty).fit()\nprint(model.summary())\n\n```\nTest of means:\n\nBlueprinty customers have significantly more patents than non-customers on average (4.13 vs. 3.47), and the difference is statistically significant (p < 0.001). However, this is an unadjusted comparison and may be influenced by other factors like age or region.\n\n\n1. Customer status is assiociated with more patients: \n\niscustomer coefficient = +0.641, p < 0.001\n\nInterpretation: Controlling for age and region, Blueprinty customers have, on average, 0.64 more patents than non-customers.\n\nStatistically significant at the 0.1% level → this relationship is unlikely to be due to chance.\n\n2. Age Has a Negative Effect:\n\nage coefficient = -0.036, p < 0.001\n\nInterpretation: For each additional year of company age, patent count decreases slightly (by 0.036).\n\nThis might suggest younger companies are more innovative, or older ones already hold established portfolios.\n\n3. Region Doesn’t Seem to Matter Much\nAll region coefficients are statistically insignificant (p > 0.05).\n\nNo strong evidence that region has a meaningful effect on patent count in this sample.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n_todo: Write down mathematically the likelihood for_ $Y \\sim \\text{Poisson}(\\lambda)$. Note that $f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!$.\n\n_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\n\n```\npoisson_loglikelihood <- function(lambda, Y){\n   ...\n}\n```\n\n_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._\n\n_todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which \"feels right\" because the mean of a Poisson distribution is lambda._\n\n_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln  \nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda <= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\n# Use actual Y values from the blueprinty dataset\nY = blueprinty['patents'].values\n\n# Lambda range for plotting\nlambdas = np.linspace(0.1, 20, 200)\nlogliks = [poisson_loglikelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambdas, logliks)\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n# We minimize the negative log-likelihood\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = optimize.minimize_scalar(neg_loglik, bounds=(0.01, 50), method='bounded')\nlambda_mle = result.x\n\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y: {np.mean(Y):.4f}\")\n\n```\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\\lambda_i = e^{X_i'\\beta}$. _For example:_\n\n```\npoisson_regression_likelihood <- function(beta, Y, X){\n   ...\n}\n```\n\n_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._\n\n_todo: Check your results using R's glm() function or Python sm.GLM() function._\n\n_todo: Interpret the results._ \n\n_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._\n\n```{python}\nimport numpy as np\nimport pandas as pd\nimport scipy.special\nimport scipy.optimize\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Poisson\nfrom scipy import stats\n\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.array(beta)\n    Y = np.array(Y)\n    X = np.array(X)\n    \n    linear_pred = X.dot(beta)\n    \n    linear_pred = np.clip(linear_pred, -30, 30)\n    \n    lambda_i = np.exp(linear_pred)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i + 1e-10) - lambda_i - scipy.special.gammaln(Y + 1))\n    \n    return log_likelihood\n\ndef negative_poisson_loglikelihood(beta, Y, X):\n    return -poisson_loglikelihood(beta, Y, X)\n\nblueprinty['age_squared'] = blueprinty['age'].astype(float) ** 2\nregion_dummies = pd.get_dummies(blueprinty['region'], prefix='region', drop_first=True)\n\nX_data = pd.DataFrame()\nX_data['age'] = blueprinty['age'].astype(float)\nX_data['age_squared'] = blueprinty['age_squared'].astype(float)\nX_data['iscustomer'] = blueprinty['iscustomer'].astype(float)\n\nfor col in region_dummies.columns:\n    X_data[col] = region_dummies[col].astype(float)\n\nX = sm.add_constant(X_data)\nY = blueprinty['patents'].astype(float)\n\nX_array = np.asarray(X)\nY_array = np.asarray(Y)\n\npoisson_model = sm.GLM(Y_array, X_array, family=Poisson())\npoisson_results = poisson_model.fit()\ninitial_beta = poisson_results.params\n\nresult = scipy.optimize.minimize(\n    negative_poisson_loglikelihood,\n    initial_beta,\n    args=(Y_array, X_array),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_mle = result.x\n\nstd_errors = poisson_results.bse\n\ncolumn_names = ['Intercept', 'Age', 'Age²']\nregion_cols = list(region_dummies.columns)\ncolumn_names.extend(region_cols)\ncolumn_names.append('Customer')\n\ncomparison_df = pd.DataFrame({\n    'Manual Coefficient': beta_mle,\n    'Statsmodels Coefficient': poisson_results.params,\n    'Std. Error': std_errors,\n    'z-value': poisson_results.params / std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(poisson_results.params / std_errors)))\n})\ncomparison_df.index = column_names[:len(beta_mle)]\n\nprint(\"Poisson Regression Results (Comparison):\")\nprint(comparison_df)\n\niscustomer_idx = list(X.columns).index('iscustomer')\ncustomer_effect = np.exp(poisson_results.params[iscustomer_idx]) - 1\nprint(f\"\\nEffect of being a Blueprinty customer: {customer_effect:.4f}\")\nprint(f\"Blueprinty customers are associated with a {customer_effect*100:.2f}% increase in patent count.\")\n\nfrom scipy.optimize import approx_fprime\n\ndef hessian(func, x, *args):\n    n = len(x)\n    h = 1e-5 \n    hessy = np.zeros((n, n))\n    \n    def grad(x, *args):\n        return approx_fprime(x, func, h, *args)\n    \n    for i in range(n):\n        x_plus = x.copy()\n        x_plus[i] += h\n        grad_plus = grad(x_plus, *args)\n        \n        grad_x = grad(x, *args)\n        \n        hessy[i] = (grad_plus - grad_x) / h\n    \n    hessy = (hessy + hessy.T) / 2\n    \n    return hessy\n\nhessy = hessian(negative_poisson_loglikelihood, beta_mle, Y_array, X_array)\n\ncov_matrix = np.linalg.inv(hessy)\nmanual_std_errors = np.sqrt(np.diag(cov_matrix))\n\nmanual_results_df = pd.DataFrame({\n    'Coefficient': beta_mle,\n    'Manual Std. Error': manual_std_errors,\n    'Statsmodels Std. Error': std_errors,\n    'z-value': beta_mle / manual_std_errors,\n    'p-value': 2 * (1 - stats.norm.cdf(np.abs(beta_mle / manual_std_errors)))\n})\nmanual_results_df.index = column_names[:len(beta_mle)]\n\nprint(\"\\nPoisson Regression Results with MSE:\")\nprint(manual_results_df)\n```\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n``` {python}\n\n\n\nvars_needed = [\n    'number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable'\n]\n\nairbnb_clean = airbnb[vars_needed].dropna()\n\n\nprint(airbnb_clean.info())\nprint(airbnb_clean.describe())\n\n# Convert price to numeric (if it's a string with $ or ,)\nairbnb_clean[\"price\"] = (\n    airbnb_clean[\"price\"]\n    .replace('[\\$,]', '', regex=True)\n    .astype(float)\n)\n\n# Convert 'instant_bookable' to binary\nairbnb_clean[\"instant_bookable\"] = airbnb_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n\n# Distribution of number of reviews\nsns.histplot(airbnb_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.show()\n\n# Boxplot of reviews by room type\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=airbnb_clean)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Fit the model\nmodel = smf.glm(\n    formula=\"\"\"\n        number_of_reviews ~ room_type + bathrooms + bedrooms + price +\n        review_scores_cleanliness + review_scores_location +\n        review_scores_value + instant_bookable\n    \"\"\",\n    data=airbnb_clean,\n    family=sm.families.Poisson()\n).fit()\n\nprint(model.summary())\n\n```\n\nUsing the number of reviews as a proxy for bookings, we built a Poisson regression model to examine how listing characteristics relate to booking volume. We found that listings which are instantly bookable receive about 40% more reviews, making this the strongest positive predictor. Higher cleanliness scores also lead to significantly more reviews, with each 1-point increase associated with a 12% gain. Additional bedrooms slightly increase bookings (about 8% per bedroom), while more bathrooms surprisingly reduce them by about 12%. Shared rooms receive 22% fewer reviews than entire homes, and private rooms show no meaningful difference. Price has a negligible effect. Interestingly, higher location and value scores are associated with fewer reviews, which may reflect confounding rather than true negative effects. Overall, features that signal convenience and cleanliness appear to drive more bookings, while room type and size also play meaningful roles"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","theme":["cosmo","brand"],"title":"Poisson Regression Examples","author":"Eleanor Amidei","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}